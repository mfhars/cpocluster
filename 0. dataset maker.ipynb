{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the data\n",
    "country_mapping = pd.read_excel(r\"C:\\Users\\pingk\\Downloads\\fadhli nitip\\country_mapping.xlsx\")\n",
    "country_2digit = pd.read_excel(r\"C:\\Users\\pingk\\Downloads\\fadhli nitip\\country_2digit.xlsx\")\n",
    "provincial_code = pd.read_excel(r\"C:\\Users\\pingk\\Downloads\\fadhli nitip\\provincial code.xlsx\")\n",
    "three_group_precipitation = pd.read_excel(r\"C:\\Users\\pingk\\Downloads\\fadhli nitip\\three_group_precipitation.xlsx\")\n",
    "four_group_precipitation = pd.read_excel(r\"C:\\Users\\pingk\\Downloads\\fadhli nitip\\four_group_precipitation.xlsx\")\n",
    "island_mapping = pd.read_excel(r\"C:\\Users\\pingk\\Downloads\\fadhli nitip\\island code.xlsx\")\n",
    "thai_mapping = pd.read_excel(r\"C:\\Users\\pingk\\Downloads\\fadhli nitip\\thai_vs_nothai.xlsx\")\n",
    "\n",
    "data_rt4 = pd.read_csv(r\"C:\\Users\\pingk\\Downloads\\fadhli nitip\\fadhli nitip\\csv\\data_rt4_raw_nodrop_removedbadfiles.csv\")\n",
    "data_rt1 = pd.read_csv(r\"C:\\Users\\pingk\\Downloads\\fadhli nitip\\fadhli nitip\\csv\\data_rt1_raw_nodrop.csv\")\n",
    "data_rt3a = pd.read_csv(r\"C:\\Users\\pingk\\Downloads\\fadhli nitip\\fadhli nitip\\csv\\data_rt3_raw_nodrop_a.csv\")\n",
    "data_rt3b = pd.read_csv(r\"C:\\Users\\pingk\\Downloads\\fadhli nitip\\fadhli nitip\\csv\\data_rt3_raw_nodrop_b.csv\")\n",
    "data_rt6 = pd.read_csv(r\"C:\\Users\\pingk\\Downloads\\fadhli nitip\\fadhli nitip\\csv\\data_rt6_raw_nodrop.csv\")\n",
    "data_rt11 = pd.read_csv(r\"C:\\Users\\pingk\\Downloads\\fadhli nitip\\fadhli nitip\\csv\\data_rt11_raw_nodrop.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Data:\n",
      "  prov_char  country_code country_char               wavenumber   649.893  \\\n",
      "0       BBX             1           ID  ID-BBX-068-2307-031_001  0.016094   \n",
      "1       BBX             1           ID  ID-BBX-068-2307-031_002  0.013664   \n",
      "2       BBX             1           ID  ID-BBX-068-2307-031_003  0.015954   \n",
      "3       BBX             1           ID  ID-BBX-068-2307-031_004  0.013254   \n",
      "4       BBX             1           ID  ID-BBX-068-2307-031_005  0.014003   \n",
      "\n",
      "    650.376   650.858    651.34   651.822   652.304  ...  3995.783  3996.265  \\\n",
      "0  0.016946  0.017469  0.017538  0.017089  0.016154  ...  0.000476  0.000609   \n",
      "1  0.013097  0.012600  0.012208  0.011938  0.011801  ...  0.000951  0.001064   \n",
      "2  0.016134  0.015986  0.015469  0.014614  0.013532  ...  0.000755  0.000871   \n",
      "3  0.012653  0.011752  0.010722  0.009749  0.008982  ...  0.001851  0.001925   \n",
      "4  0.013402  0.012824  0.012274  0.011728  0.011171  ...  0.002461  0.002472   \n",
      "\n",
      "   3996.747   3997.23  3997.712  3998.194  3998.676  3999.158   3999.64  \\\n",
      "0  0.000719  0.000797  0.000843  0.000862  0.000858  0.000840  0.000813   \n",
      "1  0.001144  0.001182  0.001180  0.001151  0.001114  0.001086  0.001078   \n",
      "2  0.000928  0.000909  0.000814  0.000658  0.000470  0.000288  0.000153   \n",
      "3  0.001983  0.002019  0.002031  0.002017  0.001975  0.001903  0.001802   \n",
      "4  0.002460  0.002435  0.002404  0.002369  0.002324  0.002266  0.002196   \n",
      "\n",
      "   4000.122  \n",
      "0  0.000788  \n",
      "1  0.001095  \n",
      "2  0.000099  \n",
      "3  0.001685  \n",
      "4  0.002118  \n",
      "\n",
      "[5 rows x 6954 columns]\n",
      "  country_name  country_code\n",
      "0     Thailand             0\n",
      "1    Indonesia             1\n",
      "2     Malaysia             2\n",
      "  country_char  country_code\n",
      "0           TH             0\n",
      "1           ID             1\n",
      "2           MY             2\n",
      "  prov_char  prov_code\n",
      "0       CBI          0\n",
      "1       CCO          1\n",
      "2       CPN          2\n",
      "3       KBI          3\n",
      "4       KRI          4\n",
      "  prov_char  three_group_precip tgp_name dgp_name\n",
      "0       SKM                   0  Group 1  Group 1\n",
      "1       PKN                   0  Group 1  Group 1\n",
      "2       CBI                   0  Group 1  Group 1\n",
      "3       CCO                   0  Group 1  Group 1\n",
      "4       KRI                   0  Group 1  Group 1\n",
      "  prov_char  four_group_precip fgp_name\n",
      "0       SKM                  0  Group 1\n",
      "1       PKN                  0  Group 1\n",
      "2       CBI                  0  Group 1\n",
      "3       CCO                  0  Group 1\n",
      "4       KRI                  0  Group 1\n",
      "  prov_char  island_code\n",
      "0       CBI            0\n",
      "1       CCO            0\n",
      "2       CPN            0\n",
      "3       KBI            0\n",
      "4       KRI            0\n",
      "  country_char  th_vs_noth thnoth_name\n",
      "0           TH           0        Thai\n",
      "1           ID           1    Non-Thai\n",
      "2           MY           1    Non-Thai\n"
     ]
    }
   ],
   "source": [
    "# Inspect the first few rows of each dataset\n",
    "print(\"Raw Data:\")\n",
    "print(data_rt4.head())\n",
    "print(country_mapping.head())\n",
    "print(country_2digit.head())\n",
    "print(provincial_code.head())\n",
    "print(three_group_precipitation.head())\n",
    "print(four_group_precipitation.head())\n",
    "print(island_mapping.head())\n",
    "print(thai_mapping.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge data_rt4 with provincial_code on 'prov_char'\n",
    "data_rt4_with_prov_code = pd.merge(data_rt6, provincial_code, how='left', left_on='prov_char', right_on='prov_char')\n",
    "\n",
    "# Merge the result with three_group_precipitation\n",
    "data_rt4_with_3precip = pd.merge(data_rt4_with_prov_code, three_group_precipitation, how='left', left_on='prov_char', right_on='prov_char')\n",
    "\n",
    "# Merge the result with four_group_precipitation\n",
    "data_rt4_with_3p4p = pd.merge(data_rt4_with_3precip, four_group_precipitation, how='left', left_on='prov_char', right_on='prov_char')\n",
    "\n",
    "# Merge the result with country_mapping\n",
    "data_rt4_with_countryname = pd.merge(data_rt4_with_3p4p, country_mapping, how='left', left_on='country_code', right_on='country_code')\n",
    "\n",
    "# Merge the result with thai_mapping\n",
    "data_rt4_with_thai = pd.merge(data_rt4_with_countryname, thai_mapping, how='left', left_on='country_char', right_on='country_char')\n",
    "\n",
    "# Merge the result with island_mapping\n",
    "data_rt4_with_sixcat = pd.merge(data_rt4_with_thai, island_mapping, how='left', left_on='prov_char', right_on='prov_char')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data with Additional Groupings:\n",
      "  prov_char  country_code country_char               wavenumber   649.893  \\\n",
      "0       BBX             1           ID  ID-BBX-068-2306-011_001  0.013426   \n",
      "1       BBX             1           ID  ID-BBX-068-2306-011_002  0.015879   \n",
      "2       BBX             1           ID  ID-BBX-068-2306-011_003  0.014529   \n",
      "3       BBX             1           ID  ID-BBX-068-2306-011_004  0.011612   \n",
      "4       BBX             1           ID  ID-BBX-068-2306-011_005  0.018775   \n",
      "\n",
      "    650.376   650.858    651.34   651.822   652.304  ...  prov_code  \\\n",
      "0  0.014513  0.015536  0.016657  0.018048  0.019807  ...         17   \n",
      "1  0.016232  0.016136  0.015821  0.015579  0.015664  ...         17   \n",
      "2  0.014121  0.013155  0.011991  0.011024  0.010581  ...         17   \n",
      "3  0.010349  0.008967  0.007803  0.007213  0.007471  ...         17   \n",
      "4  0.018823  0.018383  0.017739  0.017201  0.016998  ...         17   \n",
      "\n",
      "   three_group_precip  tgp_name  dgp_name  four_group_precip  fgp_name  \\\n",
      "0                 2.0   Group 3   Group 2                2.0   Group 3   \n",
      "1                 2.0   Group 3   Group 2                2.0   Group 3   \n",
      "2                 2.0   Group 3   Group 2                2.0   Group 3   \n",
      "3                 2.0   Group 3   Group 2                2.0   Group 3   \n",
      "4                 2.0   Group 3   Group 2                2.0   Group 3   \n",
      "\n",
      "   country_name  th_vs_noth  thnoth_name  island_code  \n",
      "0     Indonesia           1     Non-Thai            1  \n",
      "1     Indonesia           1     Non-Thai            1  \n",
      "2     Indonesia           1     Non-Thai            1  \n",
      "3     Indonesia           1     Non-Thai            1  \n",
      "4     Indonesia           1     Non-Thai            1  \n",
      "\n",
      "[5 rows x 6964 columns]\n"
     ]
    }
   ],
   "source": [
    "# Inspect the first few rows of each dataset\n",
    "print(\"Data with Additional Groupings:\")\n",
    "print(data_rt4_with_sixcat.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rt4_with_sixcat.to_csv('C:/Users/pingk/Downloads/fadhli nitip/data_rt4.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     prov_char  country_code country_char                   wavenumber  \\\n",
      "0          BBX             1           ID      ID-BBX-068-2306-011_001   \n",
      "1          BBX             1           ID      ID-BBX-068-2306-011_002   \n",
      "2          BBX             1           ID      ID-BBX-068-2306-011_003   \n",
      "3          BBX             1           ID      ID-BBX-068-2306-011_004   \n",
      "4          BBX             1           ID      ID-BBX-068-2306-011_005   \n",
      "...        ...           ...          ...                          ...   \n",
      "1795       TRG             0           TH  TH-TRG-018-2311-001_009 (2)   \n",
      "1796       TRG             0           TH      TH-TRG-018-2311-001_009   \n",
      "1797       TRG             0           TH      TH-TRG-018-2311-001_010   \n",
      "1798       TRG             0           TH      TH-TRG-018-2311-001_011   \n",
      "1799       TRG             0           TH      TH-TRG-018-2311-001_109   \n",
      "\n",
      "       649.893   650.376   650.858    651.34   651.822   652.304  ...  \\\n",
      "0     0.013426  0.014513  0.015536  0.016657  0.018048  0.019807  ...   \n",
      "1     0.015879  0.016232  0.016136  0.015821  0.015579  0.015664  ...   \n",
      "2     0.014529  0.014121  0.013155  0.011991  0.011024  0.010581  ...   \n",
      "3     0.011612  0.010349  0.008967  0.007803  0.007213  0.007471  ...   \n",
      "4     0.018775  0.018823  0.018383  0.017739  0.017201  0.016998  ...   \n",
      "...        ...       ...       ...       ...       ...       ...  ...   \n",
      "1795  0.015040  0.014318  0.013693  0.013365  0.013400  0.013690  ...   \n",
      "1796  0.015629  0.018361  0.020620  0.022051  0.022396  0.021607  ...   \n",
      "1797  0.007017  0.006969  0.007070  0.007398  0.007966  0.008694  ...   \n",
      "1798  0.009850  0.011517  0.013391  0.015410  0.017489  0.019492  ...   \n",
      "1799  0.009658  0.009609  0.009675  0.010137  0.011243  0.013074  ...   \n",
      "\n",
      "      prov_code  three_group_precip  tgp_name  dgp_name  four_group_precip  \\\n",
      "0            17                 2.0   Group 3   Group 2                2.0   \n",
      "1            17                 2.0   Group 3   Group 2                2.0   \n",
      "2            17                 2.0   Group 3   Group 2                2.0   \n",
      "3            17                 2.0   Group 3   Group 2                2.0   \n",
      "4            17                 2.0   Group 3   Group 2                2.0   \n",
      "...         ...                 ...       ...       ...                ...   \n",
      "1795         16                 1.0   Group 2   Group 1                1.0   \n",
      "1796         16                 1.0   Group 2   Group 1                1.0   \n",
      "1797         16                 1.0   Group 2   Group 1                1.0   \n",
      "1798         16                 1.0   Group 2   Group 1                1.0   \n",
      "1799         16                 1.0   Group 2   Group 1                1.0   \n",
      "\n",
      "      fgp_name  country_name  th_vs_noth  thnoth_name  island_code  \n",
      "0      Group 3     Indonesia           1     Non-Thai            1  \n",
      "1      Group 3     Indonesia           1     Non-Thai            1  \n",
      "2      Group 3     Indonesia           1     Non-Thai            1  \n",
      "3      Group 3     Indonesia           1     Non-Thai            1  \n",
      "4      Group 3     Indonesia           1     Non-Thai            1  \n",
      "...        ...           ...         ...          ...          ...  \n",
      "1795   Group 2      Thailand           0         Thai            0  \n",
      "1796   Group 2      Thailand           0         Thai            0  \n",
      "1797   Group 2      Thailand           0         Thai            0  \n",
      "1798   Group 2      Thailand           0         Thai            0  \n",
      "1799   Group 2      Thailand           0         Thai            0  \n",
      "\n",
      "[1800 rows x 6964 columns]\n"
     ]
    }
   ],
   "source": [
    "print(data_rt4_with_sixcat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       649.893   650.376   650.858    651.34   651.822   652.304   652.786  \\\n",
      "0     0.013426  0.014513  0.015536  0.016657  0.018048  0.019807  0.021896   \n",
      "1     0.015879  0.016232  0.016136  0.015821  0.015579  0.015664  0.016184   \n",
      "2     0.014529  0.014121  0.013155  0.011991  0.011024  0.010581  0.010817   \n",
      "3     0.011612  0.010349  0.008967  0.007803  0.007213  0.007471  0.008675   \n",
      "4     0.018775  0.018823  0.018383  0.017739  0.017201  0.016998  0.017179   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "1795  0.015040  0.014318  0.013693  0.013365  0.013400  0.013690  0.014005   \n",
      "1796  0.015629  0.018361  0.020620  0.022051  0.022396  0.021607  0.019884   \n",
      "1797  0.007017  0.006969  0.007070  0.007398  0.007966  0.008694  0.009436   \n",
      "1798  0.009850  0.011517  0.013391  0.015410  0.017489  0.019492  0.021233   \n",
      "1799  0.009658  0.009609  0.009675  0.010137  0.011243  0.013074  0.015481   \n",
      "\n",
      "       653.268    653.75   654.232  ...  3995.783  3996.265  3996.747  \\\n",
      "0     0.024131  0.026214  0.027845  ...  0.000110 -0.000097 -0.000264   \n",
      "1     0.017071  0.018081  0.018906  ...  0.000163  0.000145  0.000128   \n",
      "2     0.011716  0.013107  0.014753  ... -0.000525 -0.000754 -0.000898   \n",
      "3     0.010722  0.013306  0.016015  ... -0.000340 -0.000305 -0.000164   \n",
      "4     0.017608  0.018002  0.018075  ... -0.000486 -0.000490 -0.000383   \n",
      "...        ...       ...       ...  ...       ...       ...       ...   \n",
      "1795  0.014054  0.013585  0.012486  ... -0.011423 -0.011481 -0.011517   \n",
      "1796  0.017644  0.015419  0.013684  ... -0.001690 -0.001941 -0.002167   \n",
      "1797  0.009993  0.010159  0.009776  ... -0.013381 -0.013075 -0.012774   \n",
      "1798  0.022478  0.022989  0.022593  ... -0.003892 -0.003743 -0.003599   \n",
      "1799  0.018095  0.020413  0.021989  ... -0.010340 -0.010335 -0.010347   \n",
      "\n",
      "       3997.23  3997.712  3998.194  3998.676  3999.158   3999.64  4000.122  \n",
      "0    -0.000385 -0.000456 -0.000479 -0.000466 -0.000435 -0.000405 -0.000390  \n",
      "1     0.000103  0.000067  0.000028  0.000005  0.000023  0.000104  0.000254  \n",
      "2    -0.000966 -0.000977 -0.000955 -0.000920 -0.000882 -0.000841 -0.000793  \n",
      "3     0.000053  0.000299  0.000526  0.000693  0.000785  0.000811  0.000799  \n",
      "4    -0.000191  0.000041  0.000262  0.000426  0.000514  0.000532  0.000508  \n",
      "...        ...       ...       ...       ...       ...       ...       ...  \n",
      "1795 -0.011484 -0.011339 -0.011056 -0.010646 -0.010158 -0.009673 -0.009283  \n",
      "1796 -0.002322 -0.002387 -0.002366 -0.002287 -0.002184 -0.002091 -0.002033  \n",
      "1797 -0.012507 -0.012279 -0.012078 -0.011884 -0.011687 -0.011493 -0.011328  \n",
      "1798 -0.003475 -0.003365 -0.003249 -0.003108 -0.002936 -0.002743 -0.002558  \n",
      "1799 -0.010337 -0.010263 -0.010091 -0.009817 -0.009470 -0.009108 -0.008805  \n",
      "\n",
      "[1800 rows x 6950 columns]\n"
     ]
    }
   ],
   "source": [
    "# Print the filtered numeric columns in one line\n",
    "a = data_rt4_with_sixcat.drop(\n",
    "        columns=[\n",
    "            col for col in data_rt4_with_sixcat.columns \n",
    "            if col in ['country_code', 'prov_code', 'three_group_precip', 'four_group_precip', 'th_vs_noth', 'island_code'] \n",
    "            #or (col.replace('.', '', 1).isdigit() and any(float(col) >= start and float(col) < end for start, end in [(3100, 4001), (1870, 2700)]))\n",
    "            #or (col.replace('.', '', 1).isdigit() and 822.01 <= float(col) < 879.99)\n",
    "        ]\n",
    "    ).select_dtypes(include=['number'])\n",
    "\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input your dataset here\n",
    "\n",
    "asik  = data_rt4_with_sixcat.drop(\n",
    "        columns=[\n",
    "            col for col in data_rt4_with_sixcat.columns \n",
    "            if col in ['country_code', 'prov_code', 'three_group_precip', 'four_group_precip', 'th_vs_noth', 'island_code'] \n",
    "            #or (col.replace('.', '', 1).isdigit() and any(float(col) >= start and float(col) < end for start, end in [(3100, 4001), (1870, 2700)]))\n",
    "            #or (col.replace('.', '', 1).isdigit() and 822.01 <= float(col) < 879.99)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "asik.to_csv('C:/Users/pingk/Downloads/fadhli nitip/asik6a.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA MANIPULATION FUNCTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    # Select numeric columns\n",
    "    numeric_cols = df.select_dtypes(include=['number']).columns\n",
    "    df_numeric = df[numeric_cols]\n",
    "    \n",
    "    # Impute missing values\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    df_imputed = pd.DataFrame(imputer.fit_transform(df_numeric), columns=df_numeric.columns, index=df.index)\n",
    "    \n",
    "    # Log transform the data to reduce skewness\n",
    "    df_log_transformed = np.log1p(df_imputed)\n",
    "    \n",
    "    # Scale the data using RobustScaler\n",
    "    scaler = RobustScaler()\n",
    "    df_scaled = pd.DataFrame(scaler.fit_transform(df_log_transformed), columns=df_numeric.columns, index=df.index)\n",
    "    \n",
    "    # Replace the original numeric columns with the processed ones\n",
    "    df[numeric_cols] = df_scaled\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select Numeric Columns: Isolates numeric columns for processing.\n",
    "\n",
    "Impute Missing Values: Replaces missing values with the mean of each column.\n",
    "\n",
    "Log Transformation: Reduces skewness in the data.\n",
    "\n",
    "Scaling: Uses RobustScaler to scale the data, which is less sensitive to outliers.\n",
    "\n",
    "Update DataFrame: Updates the original DataFrame with the processed numeric columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Remove Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(df):\n",
    "    # Select numeric columns\n",
    "    numeric_cols = df.select_dtypes(include=['number']).columns\n",
    "    df_numeric = df[numeric_cols]\n",
    "    \n",
    "    iso = IsolationForest(contamination=0.05)\n",
    "    yhat = iso.fit_predict(df_numeric)\n",
    "    mask = yhat != -1\n",
    "    df_cleaned = df[mask].reset_index(drop=True)\n",
    "    return df_cleaned\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select Numeric Columns: Isolates numeric columns for processing.\n",
    "Isolation Forest: Identifies and removes outliers using IsolationForest.\n",
    "Filter Data: Filters out the identified outliers and resets the index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Drop Columns Within Specified Ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_columns_within_ranges(df, ranges):\n",
    "    def is_within_ranges(col_name, ranges):\n",
    "        try:\n",
    "            col_value = float(col_name)\n",
    "            return any(start <= col_value < end for start, end in ranges)\n",
    "        except ValueError:\n",
    "            return False\n",
    "\n",
    "    columns_to_drop = [col for col in df.columns if is_within_ranges(col, ranges)]\n",
    "    df_dropped = df.drop(columns=columns_to_drop)\n",
    "    return df_dropped\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "is_within_ranges: Checks if a column name falls within any specified numeric ranges.\n",
    "Drop Columns: Drops columns that fall within the specified ranges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main Data Processing and Model Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. Load Data and Drop Specific Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the ranges to drop\n",
    "ranges_to_drop = [\n",
    "    (3100, 4001), \n",
    "    (1870, 2700),\n",
    "    #(822.01, 879.99)\n",
    "]\n",
    "\n",
    "# Drop columns within the specified ranges\n",
    "data_filtered = drop_columns_within_ranges(asik, ranges_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data: Reads the dataset.\n",
    "Drop Columns: Drops columns within specified ranges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Preprocess and Remove Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "data_preprocessed = preprocess_data(data_filtered)\n",
    "\n",
    "# Remove outliers\n",
    "data_no_outliers = remove_outliers(data_preprocessed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess Data: Imputes missing values, log transforms, and scales the data.\n",
    "Remove Outliers: Removes outliers using Isolation Forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KMeans Clustering and PCA with Specific Samples Left Out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. Function Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_pca_specific_samples(df, leave_out_prov_chars, n_clusters=3, n_components=2):\n",
    "    # Separate the samples to leave out\n",
    "    leave_out_samples = df[df['prov_char'].isin(leave_out_prov_chars)]\n",
    "    remaining_samples = df[~df['prov_char'].isin(leave_out_prov_chars)]\n",
    "    \n",
    "    # Select numeric columns for clustering and PCA\n",
    "    numeric_cols = remaining_samples.select_dtypes(include=['number']).columns\n",
    "    remaining_samples_numeric = remaining_samples[numeric_cols]\n",
    "    leave_out_samples_numeric = leave_out_samples[numeric_cols]\n",
    "    \n",
    "    # Perform KMeans clustering on the remaining samples\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    clusters = kmeans.fit_predict(remaining_samples_numeric)\n",
    "    \n",
    "    # Add cluster labels to the remaining samples DataFrame\n",
    "    remaining_samples['Cluster'] = clusters\n",
    "    \n",
    "    # Apply PCA on the remaining samples\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca_result_remaining = pca.fit_transform(remaining_samples_numeric)\n",
    "    \n",
    "    # Add PCA components to the remaining samples DataFrame\n",
    "    remaining_samples[f'PC1'] = pca_result_remaining[:, 0]\n",
    "    remaining_samples[f'PC2'] = pca_result_remaining[:, 1]\n",
    "    \n",
    "    # Transform the leave out samples using the PCA model fitted on the remaining samples\n",
    "    pca_result_leave_out = pca.transform(leave_out_samples_numeric)\n",
    "    \n",
    "    # Predict clusters for the leave out samples\n",
    "    leave_out_clusters = kmeans.predict(leave_out_samples_numeric)\n",
    "    \n",
    "    # Add PCA components and cluster labels to the leave out samples DataFrame\n",
    "    leave_out_samples[f'PC1'] = pca_result_leave_out[:, 0]\n",
    "    leave_out_samples[f'PC2'] = pca_result_leave_out[:, 1]\n",
    "    leave_out_samples['Cluster'] = leave_out_clusters\n",
    "    \n",
    "    # Concatenate the remaining samples and leave out samples\n",
    "    final_df = pd.concat([remaining_samples, leave_out_samples])\n",
    "    \n",
    "    return final_df, pca\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
