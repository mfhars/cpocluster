{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "info_A1 = pd.read_csv(r\"C:\\Users\\pingk\\Downloads\\fadhli nitip\\asik.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  prov_char country_char               wavenumber   649.893   650.376  \\\n",
      "0       BBX           ID  ID-BBX-068-2307-031_001  0.384336  0.623262   \n",
      "1       BBX           ID  ID-BBX-068-2307-031_002 -0.334257 -0.516665   \n",
      "2       BBX           ID  ID-BBX-068-2307-031_003  0.343061  0.382855   \n",
      "3       BBX           ID  ID-BBX-068-2307-031_004 -0.455392 -0.648278   \n",
      "4       BBX           ID  ID-BBX-068-2307-031_005 -0.233804 -0.426464   \n",
      "\n",
      "    650.858    651.34   651.822   652.304   652.786  ...  3998.194  3998.676  \\\n",
      "0  0.790731  0.849385  0.765928  0.530749  0.169306  ... -0.124608 -0.124839   \n",
      "1 -0.666844 -0.774808 -0.833336 -0.839365 -0.799079  ...  0.001292 -0.013375   \n",
      "2  0.346922  0.218928 -0.002596 -0.294666 -0.617047  ... -0.213015 -0.294168   \n",
      "3 -0.920590 -1.227561 -1.512727 -1.726495 -1.851537  ...  0.377603  0.361953   \n",
      "4 -0.599842 -0.754752 -0.898472 -1.037614 -1.173077  ...  0.530364  0.514048   \n",
      "\n",
      "   3999.158   3999.64  4000.122  tgp_name  dgp_name  fgp_name  country_name  \\\n",
      "0 -0.132447 -0.144087 -0.155394   Group 3   Group 2   Group 3     Indonesia   \n",
      "1 -0.025058 -0.028218 -0.020505   Group 3   Group 2   Group 3     Indonesia   \n",
      "2 -0.373631 -0.433256 -0.457455   Group 3   Group 2   Group 3     Indonesia   \n",
      "3  0.331980  0.289022  0.237805   Group 3   Group 2   Group 3     Indonesia   \n",
      "4  0.490933  0.461127  0.427717   Group 3   Group 2   Group 3     Indonesia   \n",
      "\n",
      "   thnoth_name  \n",
      "0     Non-Thai  \n",
      "1     Non-Thai  \n",
      "2     Non-Thai  \n",
      "3     Non-Thai  \n",
      "4     Non-Thai  \n",
      "\n",
      "[5 rows x 6958 columns]\n"
     ]
    }
   ],
   "source": [
    "df = info_A1\n",
    "\n",
    "# Function to remove outliers\n",
    "def remove_outliers(df):\n",
    "    # Select numeric columns\n",
    "    numeric_cols = df.select_dtypes(include=['number']).columns\n",
    "    df_numeric = df[numeric_cols]\n",
    "    \n",
    "    iso = IsolationForest(contamination=0.05)\n",
    "    yhat = iso.fit_predict(df_numeric)\n",
    "    mask = yhat != -1\n",
    "    df_cleaned = df[mask].reset_index(drop=True)\n",
    "    return df_cleaned\n",
    "\n",
    "df = remove_outliers(df)\n",
    "\n",
    "# Standardize the numeric data\n",
    "numeric_cols = df.select_dtypes(include=['number']).columns\n",
    "scaler = StandardScaler()\n",
    "df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n",
    "\n",
    "# Identify numeric and string columns\n",
    "string_cols = df.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "\n",
    "# Check the first few rows of the prepared data\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['649.893', '650.376', '650.858', '651.34', '651.822', '652.304',\n",
       "       '652.786', '653.268', '653.75', '654.232',\n",
       "       ...\n",
       "       '3995.783', '3996.265', '3996.747', '3997.23', '3997.712', '3998.194',\n",
       "       '3998.676', '3999.158', '3999.64', '4000.122'],\n",
       "      dtype='object', length=6950)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['prov_char',\n",
       " 'country_char',\n",
       " 'wavenumber',\n",
       " 'tgp_name',\n",
       " 'dgp_name',\n",
       " 'fgp_name',\n",
       " 'country_name',\n",
       " 'thnoth_name']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    country_name  country_name_encoded\n",
      "0      Indonesia                     0\n",
      "1      Indonesia                     0\n",
      "2      Indonesia                     0\n",
      "3      Indonesia                     0\n",
      "4      Indonesia                     0\n",
      "..           ...                   ...\n",
      "410     Thailand                     2\n",
      "411     Thailand                     2\n",
      "412     Thailand                     2\n",
      "413     Thailand                     2\n",
      "414     Thailand                     2\n",
      "\n",
      "[415 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pingk\\AppData\\Local\\Temp\\ipykernel_1628\\2480830658.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['country_name_encoded'] = label_encoder.fit_transform(df['country_name'])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encode the target variable\n",
    "label_encoder = LabelEncoder()\n",
    "df['country_name_encoded'] = label_encoder.fit_transform(df['country_name'])\n",
    "\n",
    "# Check the encoded target variable\n",
    "print(df[['country_name', 'country_name_encoded']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features:\n",
      "Index(['664.839', '665.321', '665.803', '807.546', '1053.907', '1054.389',\n",
      "       '1054.871', '1055.836', '1061.139', '1062.103', '1062.585', '1063.067',\n",
      "       '1138.76', '1140.688', '1159.491', '1159.973', '1160.455', '1166.24',\n",
      "       '1167.205', '1168.169', '1168.651', '1169.133', '1175.883', '1187.936',\n",
      "       '1188.418', '1188.9', '1467.081', '1491.187', '1539.399', '1539.881',\n",
      "       '1666.195', '1668.124', '1677.284', '1677.766', '1678.248', '1678.73',\n",
      "       '1717.3', '1718.264', '1738.031', '1752.494', '1925.574', '2225.933',\n",
      "       '2226.415', '2226.897', '2895.111', '2895.593', '2916.806', '2917.77',\n",
      "       '2918.735', '2919.217'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Define the Random Forest model\n",
    "model_rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Number of features to select\n",
    "n_features_to_select = 50  # Adjust based on your needs\n",
    "\n",
    "# Initialize RFE with the model\n",
    "rfe = RFE(estimator=model_rf, n_features_to_select=n_features_to_select, step=10)\n",
    "\n",
    "# Prepare features and target\n",
    "X = df.drop(columns=['country_name', 'country_name_encoded', 'prov_char', 'country_char', 'wavenumber', 'thnoth_name', 'tgp_name', 'dgp_name', 'fgp_name'])\n",
    "y = df['country_name_encoded']\n",
    "\n",
    "# Fit RFE on the dataset\n",
    "rfe.fit(X, y)\n",
    "\n",
    "# Identify the selected features\n",
    "selected_features = X.columns[rfe.support_]\n",
    "print(\"Selected Features:\")\n",
    "print(selected_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "when `importance_getter=='auto'`, the underlying estimator SVC should have `coef_` or `feature_importances_` attribute. Either pass a fitted estimator to feature selector or call fit before calling transform.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Initialize RFE with SVM\u001b[39;00m\n\u001b[0;32m     16\u001b[0m rfe_svm \u001b[38;5;241m=\u001b[39m RFE(estimator\u001b[38;5;241m=\u001b[39mmodel_svm, n_features_to_select\u001b[38;5;241m=\u001b[39mn_features_to_select, step\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m---> 17\u001b[0m X_rfe_svm \u001b[38;5;241m=\u001b[39m \u001b[43mrfe_svm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Set up GridSearchCV\u001b[39;00m\n\u001b[0;32m     20\u001b[0m grid_search_svm \u001b[38;5;241m=\u001b[39m GridSearchCV(estimator\u001b[38;5;241m=\u001b[39mmodel_svm, param_grid\u001b[38;5;241m=\u001b[39mparam_grid, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\pingk\\OneDrive - Chulalongkorn University\\Documents\\mekargit\\cpocluster\\env\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:313\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 313\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    314\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    315\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    316\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    317\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    318\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    319\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\pingk\\OneDrive - Chulalongkorn University\\Documents\\mekargit\\cpocluster\\env\\Lib\\site-packages\\sklearn\\base.py:1101\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m   1098\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1100\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m-> 1101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32mc:\\Users\\pingk\\OneDrive - Chulalongkorn University\\Documents\\mekargit\\cpocluster\\env\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pingk\\OneDrive - Chulalongkorn University\\Documents\\mekargit\\cpocluster\\env\\Lib\\site-packages\\sklearn\\feature_selection\\_rfe.py:268\u001b[0m, in \u001b[0;36mRFE.fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit the RFE model and then the underlying estimator on the selected features.\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \n\u001b[0;32m    250\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;124;03m    Fitted estimator.\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    267\u001b[0m _raise_for_unsupported_routing(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m--> 268\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pingk\\OneDrive - Chulalongkorn University\\Documents\\mekargit\\cpocluster\\env\\Lib\\site-packages\\sklearn\\feature_selection\\_rfe.py:326\u001b[0m, in \u001b[0;36mRFE._fit\u001b[1;34m(self, X, y, step_score, **fit_params)\u001b[0m\n\u001b[0;32m    323\u001b[0m estimator\u001b[38;5;241m.\u001b[39mfit(X[:, features], y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;66;03m# Get importance and rank them\u001b[39;00m\n\u001b[1;32m--> 326\u001b[0m importances \u001b[38;5;241m=\u001b[39m \u001b[43m_get_feature_importances\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    327\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    328\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimportance_getter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransform_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msquare\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    331\u001b[0m ranks \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margsort(importances)\n\u001b[0;32m    333\u001b[0m \u001b[38;5;66;03m# for sparse case ranks is matrix\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\pingk\\OneDrive - Chulalongkorn University\\Documents\\mekargit\\cpocluster\\env\\Lib\\site-packages\\sklearn\\feature_selection\\_base.py:228\u001b[0m, in \u001b[0;36m_get_feature_importances\u001b[1;34m(estimator, getter, transform_func, norm_order)\u001b[0m\n\u001b[0;32m    226\u001b[0m         getter \u001b[38;5;241m=\u001b[39m attrgetter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeature_importances_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    227\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 228\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    229\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhen `importance_getter==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`, the underlying \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    230\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mestimator \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m should have \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    231\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`coef_` or `feature_importances_` attribute. Either \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    232\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpass a fitted estimator to feature selector or call fit \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    233\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbefore calling transform.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    234\u001b[0m         )\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    236\u001b[0m     getter \u001b[38;5;241m=\u001b[39m attrgetter(getter)\n",
      "\u001b[1;31mValueError\u001b[0m: when `importance_getter=='auto'`, the underlying estimator SVC should have `coef_` or `feature_importances_` attribute. Either pass a fitted estimator to feature selector or call fit before calling transform."
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the SVM model\n",
    "model_svm = SVC(random_state=42)\n",
    "\n",
    "# Parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "# Initialize RFE with SVM\n",
    "rfe_svm = RFE(estimator=model_svm, n_features_to_select=n_features_to_select, step=10)\n",
    "X_rfe_svm = rfe_svm.fit_transform(X, y)\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid_search_svm = GridSearchCV(estimator=model_svm, param_grid=param_grid, cv=5, n_jobs=-1, scoring='accuracy')\n",
    "\n",
    "# Fit the model with the RFE-selected features\n",
    "grid_search_svm.fit(X_rfe_svm, y)\n",
    "\n",
    "# Get the best parameters and score\n",
    "print(\"Best parameters:\", grid_search_svm.best_params_)\n",
    "print(\"Best cross-validation score:\", grid_search_svm.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "# Define the best SVM model with the obtained parameters\n",
    "best_svm = SVC(**grid_search_svm.best_params_, random_state=42)\n",
    "\n",
    "# Perform 10-fold cross-validation with the tuned SVM model\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize lists to store results\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "y_true_all = []\n",
    "y_pred_all = []\n",
    "\n",
    "# Apply 10-fold cross-validation\n",
    "for train_index, test_index in kf.split(X_rfe_svm):\n",
    "    X_train, X_test = X_rfe_svm[train_index], X_rfe_svm[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Fit the model\n",
    "    best_svm.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict the train and test samples\n",
    "    y_train_pred = best_svm.predict(X_train)\n",
    "    y_test_pred = best_svm.predict(X_test)\n",
    "    \n",
    "    # Calculate and store train and test accuracies\n",
    "    train_accuracies.append(accuracy_score(y_train, y_train_pred))\n",
    "    test_accuracies.append(accuracy_score(y_test, y_test_pred))\n",
    "    \n",
    "    # Store the prediction and actual value\n",
    "    y_true_all.extend(y_test)\n",
    "    y_pred_all.extend(y_test_pred)\n",
    "\n",
    "# Calculate average accuracies\n",
    "avg_train_accuracy = np.mean(train_accuracies)\n",
    "avg_test_accuracy = np.mean(test_accuracies)\n",
    "\n",
    "print(f'10-Fold CV with Tuned SVM - Average Train Accuracy: {avg_train_accuracy}')\n",
    "print(f'10-Fold CV with Tuned SVM - Average Test Accuracy: {avg_test_accuracy}')\n",
    "\n",
    "# Generate the classification report for the overall test predictions\n",
    "report_svm_tuned = classification_report(y_true_all, y_pred_all, target_names=label_encoder.classes_, output_dict=True)\n",
    "print(\"10-Fold CV Tuned SVM Model Classification Report\")\n",
    "print(classification_report(y_true_all, y_pred_all, target_names=label_encoder.classes_))\n",
    "\n",
    "# Plot the confusion matrix\n",
    "cm_tuned_svm = confusion_matrix(y_true_all, y_pred_all)\n",
    "disp_tuned_svm = ConfusionMatrixDisplay(confusion_matrix=cm_tuned_svm, display_labels=label_encoder.classes_)\n",
    "disp_tuned_svm.plot(cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix: 10-Fold CV Tuned SVM')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the best SVM model with the obtained parameters\n",
    "best_svm = SVC(**grid_search_svm.best_params_, random_state=42)\n",
    "\n",
    "# Initialize lists to store results\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "y_true_all = []\n",
    "y_pred_all = []\n",
    "\n",
    "# Apply 40-fold cross-validation\n",
    "kf = KFold(n_splits=40, shuffle=True, random_state=42)\n",
    "\n",
    "for train_index, test_index in kf.split(X_rfe_svm):\n",
    "    X_train, X_test = X_rfe_svm[train_index], X_rfe_svm[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Fit the model\n",
    "    best_svm.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict the train and test samples\n",
    "    y_train_pred = best_svm.predict(X_train)\n",
    "    y_test_pred = best_svm.predict(X_test)\n",
    "    \n",
    "    # Calculate and store train and test accuracies\n",
    "    train_accuracies.append(accuracy_score(y_train, y_train_pred))\n",
    "    test_accuracies.append(accuracy_score(y_test, y_test_pred))\n",
    "    \n",
    "    # Store the prediction and actual value\n",
    "    y_true_all.extend(y_test)\n",
    "    y_pred_all.extend(y_test_pred)\n",
    "\n",
    "# Calculate average accuracies\n",
    "avg_train_accuracy = np.mean(train_accuracies)\n",
    "avg_test_accuracy = np.mean(test_accuracies)\n",
    "\n",
    "print(f'40-Fold CV with Tuned SVM - Average Train Accuracy: {avg_train_accuracy}')\n",
    "print(f'40-Fold CV with Tuned SVM - Average Test Accuracy: {avg_test_accuracy}')\n",
    "\n",
    "# Generate the classification report for the overall test predictions\n",
    "report_svm_tuned = classification_report(y_true_all, y_pred_all, target_names=label_encoder.classes_, output_dict=True)\n",
    "print(\"40-Fold CV Tuned SVM Model Classification Report\")\n",
    "print(classification_report(y_true_all, y_pred_all, target_names=label_encoder.classes_))\n",
    "\n",
    "# Plot the confusion matrix\n",
    "cm_tuned_svm = confusion_matrix(y_true_all, y_pred_all)\n",
    "disp_tuned_svm = ConfusionMatrixDisplay(confusion_matrix=cm_tuned_svm, display_labels=label_encoder.classes_)\n",
    "disp_tuned_svm.plot(cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix: 40-Fold CV Tuned SVM')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "\n",
    "# Define the best SVM model with the obtained parameters\n",
    "best_svm = SVC(**grid_search_svm.best_params_, random_state=42)\n",
    "\n",
    "# Initialize Leave-One-Group-Out Cross-Validation\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "# List of provinces to use as groups\n",
    "prov_codes = ['SKM', 'PLG', 'JHR', 'SSX', 'PNA', 'KTX']\n",
    "\n",
    "# Map 'prov_char' to integer labels\n",
    "groups = df['prov_char']\n",
    "\n",
    "# Filter out samples not in the selected provinces\n",
    "X_filtered = X_rfe_svm[groups != -1]\n",
    "y_filtered = y[groups != -1]\n",
    "groups_filtered = [g for g in groups if g != -1]\n",
    "\n",
    "# Initialize lists to store results\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "y_true_all = []\n",
    "y_pred_all = []\n",
    "\n",
    "# Apply LOGO-CV\n",
    "for train_index, test_index in logo.split(X_filtered, y_filtered, groups_filtered):\n",
    "    X_train, X_test = X_filtered[train_index], X_filtered[test_index]\n",
    "    y_train, y_test = y_filtered[train_index], y_filtered[test_index]\n",
    "    \n",
    "    # Fit the model\n",
    "    best_svm.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict the train and test samples\n",
    "    y_train_pred = best_svm.predict(X_train)\n",
    "    y_test_pred = best_svm.predict(X_test)\n",
    "    \n",
    "    # Calculate and store train and test accuracies\n",
    "    train_accuracies.append(accuracy_score(y_train, y_train_pred))\n",
    "    test_accuracies.append(accuracy_score(y_test, y_test_pred))\n",
    "    \n",
    "    # Store the prediction and actual value\n",
    "    y_true_all.extend(y_test)\n",
    "    y_pred_all.extend(y_test_pred)\n",
    "\n",
    "# Calculate average accuracies\n",
    "avg_train_accuracy = np.mean(train_accuracies)\n",
    "avg_test_accuracy = np.mean(test_accuracies)\n",
    "\n",
    "print(f'Leave-One-Province-Out CV with Tuned SVM - Average Train Accuracy: {avg_train_accuracy}')\n",
    "print(f'Leave-One-Province-Out CV with Tuned SVM - Average Test Accuracy: {avg_test_accuracy}')\n",
    "\n",
    "# Generate the classification report for the overall test predictions\n",
    "report_svm_logo = classification_report(y_true_all, y_pred_all, target_names=label_encoder.classes_, output_dict=True)\n",
    "print(\"Leave-One-Province-Out CV Tuned SVM Model Classification Report\")\n",
    "print(classification_report(y_true_all, y_pred_all, target_names=label_encoder.classes_))\n",
    "\n",
    "# Plot the confusion matrix\n",
    "cm_logo_svm = confusion_matrix(y_true_all, y_pred_all)\n",
    "disp_logo_svm = ConfusionMatrixDisplay(confusion_matrix=cm_logo_svm, display_labels=label_encoder.classes_)\n",
    "disp_logo_svm.plot(cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix: Leave-One-Province-Out CV Tuned SVM')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
