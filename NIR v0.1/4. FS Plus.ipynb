{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 940 entries, 0 to 939\n",
      "Columns: 245 entries, thnoth_name to 8240\n",
      "dtypes: float64(243), object(2)\n",
      "memory usage: 1.8+ MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = r\"data/data file 2b/data_1_1_der.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Display basic information about the dataset\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['thnoth_name', 'prov_char', '4680', '4676', '4672', '4668', '4664', '4660', '4656', '4652', '4648', '4644', '5508', '5504', '5500', '5496', '5492', '5488', '5484', '5480', '5476', '5472', '5468', '5464', '5460', '5456', '5452', '5448', '5444', '5440', '5436', '5432', '5428', '5424', '5420', '5416', '5412', '5408', '5404', '5400', '5396', '5392', '5388', '5384', '5380', '5376', '5372', '5368', '5364', '5360', '5356', '5352', '5348', '5344', '5340', '5336', '5332', '5328', '5324', '5320', '5316', '5312', '5308', '5304', '5300', '5296', '5292', '5288', '5284', '5280', '5276', '5272', '5268', '5264', '5260', '5256', '5252', '5248', '5244', '5240', '5236', '5232', '5228', '5224', '5220', '5216', '5212', '5208', '5204', '5200', '5196', '5192', '5188', '5184', '5180', '5176', '5172', '5168', '5164', '5160', '5156', '5152', '5148', '5144', '5140', '5136', '5132', '5128', '5124', '5120', '5116', '5112', '5108', '5104', '5100', '5096', '5092', '5088', '5084', '5080', '5076', '5072', '5068', '5064', '5060', '5056', '5052', '5048', '5044', '5040', '5036', '5032', '5028', '5024', '5020', '5016', '5012', '5008', '5004', '5000', '4996', '4992', '4988', '4984', '4980', '4976', '4972', '4968', '4964', '4960', '4956', '4952', '4948', '4944', '4940', '4936', '4932', '4928', '4924', '4920', '4916', '4912', '4908', '4904', '4900', '4896', '4892', '4888', '4884', '4880', '4876', '4872', '4868', '5824', '5820', '5816', '5812', '5808', '5804', '5800', '5796', '5792', '5788', '5784', '5780', '5776', '5772', '5768', '5764', '5760', '5756', '5752', '5748', '5744', '5740', '5736', '5732', '5728', '5724', '5720', '5716', '5712', '5708', '5704', '5700', '5696', '5692', '5688', '5684', '5680', '5676', '5672', '5668', '5664', '5660', '7096', '7092', '7088', '7084', '7080', '7076', '7072', '7068', '7064', '7060', '7208', '7204', '7200', '7196', '7192', '7188', '7184', '7180', '7176', '7172', '8276', '8272', '8268', '8264', '8260', '8256', '8252', '8248', '8244', '8240']\n"
     ]
    }
   ],
   "source": [
    "# Display column names\n",
    "column_names = data.columns.tolist()\n",
    "print(column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>4641_4681_mean</th>\n",
       "      <th>4641_4681_std</th>\n",
       "      <th>4641_4681_skew</th>\n",
       "      <th>4641_4681_kurtosis</th>\n",
       "      <th>4641_4681_sum</th>\n",
       "      <th>4641_4681_median</th>\n",
       "      <th>4641_4681_first_derivative</th>\n",
       "      <th>4641_4681_second_derivative</th>\n",
       "      <th>4641_4681_fft_real</th>\n",
       "      <th>4641_4681_fft_imag</th>\n",
       "      <th>...</th>\n",
       "      <th>8238_8278_mean</th>\n",
       "      <th>8238_8278_std</th>\n",
       "      <th>8238_8278_skew</th>\n",
       "      <th>8238_8278_kurtosis</th>\n",
       "      <th>8238_8278_sum</th>\n",
       "      <th>8238_8278_median</th>\n",
       "      <th>8238_8278_first_derivative</th>\n",
       "      <th>8238_8278_second_derivative</th>\n",
       "      <th>8238_8278_fft_real</th>\n",
       "      <th>8238_8278_fft_imag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.000664</td>\n",
       "      <td>0.002940</td>\n",
       "      <td>-3.158411</td>\n",
       "      <td>9.982080</td>\n",
       "      <td>-0.006640</td>\n",
       "      <td>0.000259</td>\n",
       "      <td>-0.001032</td>\n",
       "      <td>-0.001181</td>\n",
       "      <td>0.000259</td>\n",
       "      <td>-8.673617e-20</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000934</td>\n",
       "      <td>0.002773</td>\n",
       "      <td>-3.112939</td>\n",
       "      <td>9.770092</td>\n",
       "      <td>-0.009337</td>\n",
       "      <td>-0.000124</td>\n",
       "      <td>0.001005</td>\n",
       "      <td>-0.001038</td>\n",
       "      <td>-0.008792</td>\n",
       "      <td>1.084202e-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.000054</td>\n",
       "      <td>0.000407</td>\n",
       "      <td>-1.036213</td>\n",
       "      <td>1.110674</td>\n",
       "      <td>-0.000539</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>-0.000135</td>\n",
       "      <td>-0.000091</td>\n",
       "      <td>0.000280</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000658</td>\n",
       "      <td>0.001937</td>\n",
       "      <td>-3.060245</td>\n",
       "      <td>9.526792</td>\n",
       "      <td>-0.006576</td>\n",
       "      <td>-0.000145</td>\n",
       "      <td>0.000714</td>\n",
       "      <td>-0.000706</td>\n",
       "      <td>-0.006123</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000702</td>\n",
       "      <td>0.002824</td>\n",
       "      <td>3.102302</td>\n",
       "      <td>9.722214</td>\n",
       "      <td>0.007021</td>\n",
       "      <td>-0.000099</td>\n",
       "      <td>0.000952</td>\n",
       "      <td>0.001141</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>1.734723e-19</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000051</td>\n",
       "      <td>0.000275</td>\n",
       "      <td>0.290498</td>\n",
       "      <td>-1.055020</td>\n",
       "      <td>-0.000511</td>\n",
       "      <td>-0.000138</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>-0.000196</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000996</td>\n",
       "      <td>0.004076</td>\n",
       "      <td>3.128186</td>\n",
       "      <td>9.841952</td>\n",
       "      <td>0.009965</td>\n",
       "      <td>-0.000199</td>\n",
       "      <td>0.001390</td>\n",
       "      <td>0.001634</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000179</td>\n",
       "      <td>0.000739</td>\n",
       "      <td>2.351724</td>\n",
       "      <td>6.343585</td>\n",
       "      <td>0.001787</td>\n",
       "      <td>-0.000041</td>\n",
       "      <td>-0.000191</td>\n",
       "      <td>0.000327</td>\n",
       "      <td>0.002124</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000911</td>\n",
       "      <td>0.003818</td>\n",
       "      <td>3.128708</td>\n",
       "      <td>9.844599</td>\n",
       "      <td>0.009114</td>\n",
       "      <td>-0.000187</td>\n",
       "      <td>0.001304</td>\n",
       "      <td>0.001531</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>-1.734723e-19</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000341</td>\n",
       "      <td>0.092990</td>\n",
       "      <td>-1.463339</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>-0.000043</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>0.000506</td>\n",
       "      <td>-5.421011e-21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   4641_4681_mean  4641_4681_std  4641_4681_skew  4641_4681_kurtosis  \\\n",
       "0       -0.000664       0.002940       -3.158411            9.982080   \n",
       "1       -0.000054       0.000407       -1.036213            1.110674   \n",
       "2        0.000702       0.002824        3.102302            9.722214   \n",
       "3        0.000996       0.004076        3.128186            9.841952   \n",
       "4        0.000911       0.003818        3.128708            9.844599   \n",
       "\n",
       "   4641_4681_sum  4641_4681_median  4641_4681_first_derivative  \\\n",
       "0      -0.006640          0.000259                   -0.001032   \n",
       "1      -0.000539         -0.000024                   -0.000135   \n",
       "2       0.007021         -0.000099                    0.000952   \n",
       "3       0.009965         -0.000199                    0.001390   \n",
       "4       0.009114         -0.000187                    0.001304   \n",
       "\n",
       "   4641_4681_second_derivative  4641_4681_fft_real  4641_4681_fft_imag  ...  \\\n",
       "0                    -0.001181            0.000259       -8.673617e-20  ...   \n",
       "1                    -0.000091            0.000280        0.000000e+00  ...   \n",
       "2                     0.001141            0.000128        1.734723e-19  ...   \n",
       "3                     0.001634            0.000055        0.000000e+00  ...   \n",
       "4                     0.001531            0.000005       -1.734723e-19  ...   \n",
       "\n",
       "   8238_8278_mean  8238_8278_std  8238_8278_skew  8238_8278_kurtosis  \\\n",
       "0       -0.000934       0.002773       -3.112939            9.770092   \n",
       "1       -0.000658       0.001937       -3.060245            9.526792   \n",
       "2       -0.000051       0.000275        0.290498           -1.055020   \n",
       "3        0.000179       0.000739        2.351724            6.343585   \n",
       "4        0.000010       0.000341        0.092990           -1.463339   \n",
       "\n",
       "   8238_8278_sum  8238_8278_median  8238_8278_first_derivative  \\\n",
       "0      -0.009337         -0.000124                    0.001005   \n",
       "1      -0.006576         -0.000145                    0.000714   \n",
       "2      -0.000511         -0.000138                    0.000064   \n",
       "3       0.001787         -0.000041                   -0.000191   \n",
       "4       0.000098         -0.000043                   -0.000011   \n",
       "\n",
       "   8238_8278_second_derivative  8238_8278_fft_real  8238_8278_fft_imag  \n",
       "0                    -0.001038           -0.008792        1.084202e-20  \n",
       "1                    -0.000706           -0.006123        0.000000e+00  \n",
       "2                     0.000033           -0.000196        0.000000e+00  \n",
       "3                     0.000327            0.002124        0.000000e+00  \n",
       "4                     0.000132            0.000506       -5.421011e-21  \n",
       "\n",
       "[5 rows x 60 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.fftpack import fft\n",
    "\n",
    "# Define regions of interest\n",
    "regions_of_interest = [\n",
    "    (4641, 4681), (4867, 5510), (5657, 5826), (7057, 7097),\n",
    "    (7169, 7209), (8238, 8278)\n",
    "]\n",
    "\n",
    "# Function to extract features from regions of interest\n",
    "def extract_features(data, regions):\n",
    "    features = pd.DataFrame()\n",
    "    for start, end in regions:\n",
    "        # Identify columns that fall within the specified range\n",
    "        region_columns = [col for col in data.columns if col.isdigit() and start <= int(col) <= end]\n",
    "        if region_columns:\n",
    "            region_data = data[region_columns]\n",
    "            # Statistical features\n",
    "            features[f'{start}_{end}_mean'] = region_data.mean(axis=1)\n",
    "            features[f'{start}_{end}_std'] = region_data.std(axis=1)\n",
    "            features[f'{start}_{end}_skew'] = region_data.skew(axis=1)\n",
    "            features[f'{start}_{end}_kurtosis'] = region_data.kurt(axis=1)\n",
    "            # Aggregated features\n",
    "            features[f'{start}_{end}_sum'] = region_data.sum(axis=1)\n",
    "            features[f'{start}_{end}_median'] = region_data.median(axis=1)\n",
    "            # Derivative features\n",
    "            features[f'{start}_{end}_first_derivative'] = region_data.diff(axis=1).mean(axis=1)\n",
    "            features[f'{start}_{end}_second_derivative'] = region_data.diff(axis=1).diff(axis=1).mean(axis=1)\n",
    "            # Fourier Transform features\n",
    "            fft_features = fft(region_data, axis=1)\n",
    "            features[f'{start}_{end}_fft_real'] = np.real(fft_features).mean(axis=1)\n",
    "            features[f'{start}_{end}_fft_imag'] = np.imag(fft_features).mean(axis=1)\n",
    "    return features\n",
    "\n",
    "# Extract features\n",
    "X = extract_features(data, regions_of_interest)\n",
    "\n",
    "# Display the extracted features\n",
    "X.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>thnoth_name</th>\n",
       "      <th>prov_char</th>\n",
       "      <th>4680</th>\n",
       "      <th>4676</th>\n",
       "      <th>4672</th>\n",
       "      <th>4668</th>\n",
       "      <th>4664</th>\n",
       "      <th>4660</th>\n",
       "      <th>4656</th>\n",
       "      <th>4652</th>\n",
       "      <th>...</th>\n",
       "      <th>8276</th>\n",
       "      <th>8272</th>\n",
       "      <th>8268</th>\n",
       "      <th>8264</th>\n",
       "      <th>8260</th>\n",
       "      <th>8256</th>\n",
       "      <th>8252</th>\n",
       "      <th>8248</th>\n",
       "      <th>8244</th>\n",
       "      <th>8240</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Non-Thai</td>\n",
       "      <td>KAX</td>\n",
       "      <td>0.000259</td>\n",
       "      <td>0.000282</td>\n",
       "      <td>0.000293</td>\n",
       "      <td>0.000232</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>0.000160</td>\n",
       "      <td>0.000259</td>\n",
       "      <td>0.000356</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008792</td>\n",
       "      <td>-0.000241</td>\n",
       "      <td>-0.000542</td>\n",
       "      <td>-0.000147</td>\n",
       "      <td>0.000269</td>\n",
       "      <td>0.000194</td>\n",
       "      <td>-0.000101</td>\n",
       "      <td>-0.000236</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Non-Thai</td>\n",
       "      <td>KBX</td>\n",
       "      <td>0.000280</td>\n",
       "      <td>0.000331</td>\n",
       "      <td>0.000372</td>\n",
       "      <td>0.000278</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>-0.000126</td>\n",
       "      <td>-0.000258</td>\n",
       "      <td>-0.000302</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006123</td>\n",
       "      <td>-0.000196</td>\n",
       "      <td>-0.000510</td>\n",
       "      <td>-0.000191</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>0.000220</td>\n",
       "      <td>-0.000099</td>\n",
       "      <td>-0.000252</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Non-Thai</td>\n",
       "      <td>BBX</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>0.000159</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>-0.000210</td>\n",
       "      <td>-0.000407</td>\n",
       "      <td>-0.000519</td>\n",
       "      <td>-0.000536</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000196</td>\n",
       "      <td>-0.000143</td>\n",
       "      <td>-0.000464</td>\n",
       "      <td>-0.000232</td>\n",
       "      <td>0.000249</td>\n",
       "      <td>0.000254</td>\n",
       "      <td>-0.000132</td>\n",
       "      <td>-0.000293</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Non-Thai</td>\n",
       "      <td>SUX</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>-0.000076</td>\n",
       "      <td>-0.000323</td>\n",
       "      <td>-0.000546</td>\n",
       "      <td>-0.000668</td>\n",
       "      <td>-0.000664</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002124</td>\n",
       "      <td>-0.000161</td>\n",
       "      <td>-0.000466</td>\n",
       "      <td>-0.000230</td>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.000252</td>\n",
       "      <td>-0.000165</td>\n",
       "      <td>-0.000313</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.000407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Non-Thai</td>\n",
       "      <td>SUX</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>-0.000078</td>\n",
       "      <td>-0.000297</td>\n",
       "      <td>-0.000512</td>\n",
       "      <td>-0.000648</td>\n",
       "      <td>-0.000658</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000506</td>\n",
       "      <td>-0.000228</td>\n",
       "      <td>-0.000492</td>\n",
       "      <td>-0.000171</td>\n",
       "      <td>0.000308</td>\n",
       "      <td>0.000224</td>\n",
       "      <td>-0.000214</td>\n",
       "      <td>-0.000324</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>935</th>\n",
       "      <td>Thai</td>\n",
       "      <td>KBI</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.000185</td>\n",
       "      <td>0.000179</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>-0.000087</td>\n",
       "      <td>-0.000252</td>\n",
       "      <td>-0.000330</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001759</td>\n",
       "      <td>-0.000263</td>\n",
       "      <td>-0.000510</td>\n",
       "      <td>-0.000133</td>\n",
       "      <td>0.000296</td>\n",
       "      <td>0.000145</td>\n",
       "      <td>-0.000253</td>\n",
       "      <td>-0.000316</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>936</th>\n",
       "      <td>Thai</td>\n",
       "      <td>KBI</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-0.000068</td>\n",
       "      <td>-0.000056</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002138</td>\n",
       "      <td>-0.000398</td>\n",
       "      <td>-0.000639</td>\n",
       "      <td>-0.000079</td>\n",
       "      <td>0.000252</td>\n",
       "      <td>-0.000026</td>\n",
       "      <td>-0.000318</td>\n",
       "      <td>-0.000296</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>0.000152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>937</th>\n",
       "      <td>Thai</td>\n",
       "      <td>SNI</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>-0.000057</td>\n",
       "      <td>-0.000106</td>\n",
       "      <td>-0.000114</td>\n",
       "      <td>-0.000095</td>\n",
       "      <td>-0.000073</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002284</td>\n",
       "      <td>-0.000174</td>\n",
       "      <td>-0.000267</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000294</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>-0.000342</td>\n",
       "      <td>-0.000470</td>\n",
       "      <td>-0.000207</td>\n",
       "      <td>0.000016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>938</th>\n",
       "      <td>Thai</td>\n",
       "      <td>TRG</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-0.000061</td>\n",
       "      <td>-0.000132</td>\n",
       "      <td>-0.000203</td>\n",
       "      <td>-0.000236</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001338</td>\n",
       "      <td>-0.000106</td>\n",
       "      <td>-0.000385</td>\n",
       "      <td>-0.000189</td>\n",
       "      <td>0.000302</td>\n",
       "      <td>0.000252</td>\n",
       "      <td>-0.000234</td>\n",
       "      <td>-0.000348</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>0.000491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>939</th>\n",
       "      <td>Thai</td>\n",
       "      <td>NRT</td>\n",
       "      <td>-0.000061</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000064</td>\n",
       "      <td>-0.000125</td>\n",
       "      <td>-0.000118</td>\n",
       "      <td>-0.000051</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004223</td>\n",
       "      <td>-0.000174</td>\n",
       "      <td>-0.000530</td>\n",
       "      <td>-0.000254</td>\n",
       "      <td>0.000289</td>\n",
       "      <td>0.000285</td>\n",
       "      <td>-0.000118</td>\n",
       "      <td>-0.000233</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.000542</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>940 rows Ã— 245 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    thnoth_name prov_char      4680      4676      4672      4668      4664  \\\n",
       "0      Non-Thai       KAX  0.000259  0.000282  0.000293  0.000232  0.000153   \n",
       "1      Non-Thai       KBX  0.000280  0.000331  0.000372  0.000278  0.000077   \n",
       "2      Non-Thai       BBX  0.000128  0.000159  0.000154  0.000012 -0.000210   \n",
       "3      Non-Thai       SUX  0.000055  0.000089  0.000081 -0.000076 -0.000323   \n",
       "4      Non-Thai       SUX  0.000005  0.000047  0.000055 -0.000078 -0.000297   \n",
       "..          ...       ...       ...       ...       ...       ...       ...   \n",
       "935        Thai       KBI  0.000176  0.000185  0.000179  0.000140  0.000060   \n",
       "936        Thai       KBI  0.000041 -0.000006 -0.000068 -0.000056  0.000001   \n",
       "937        Thai       SNI -0.000012 -0.000018 -0.000057 -0.000106 -0.000114   \n",
       "938        Thai       TRG  0.000018  0.000039  0.000042 -0.000003 -0.000061   \n",
       "939        Thai       NRT -0.000061 -0.000032 -0.000013 -0.000064 -0.000125   \n",
       "\n",
       "         4660      4656      4652  ...      8276      8272      8268  \\\n",
       "0    0.000160  0.000259  0.000356  ... -0.008792 -0.000241 -0.000542   \n",
       "1   -0.000126 -0.000258 -0.000302  ... -0.006123 -0.000196 -0.000510   \n",
       "2   -0.000407 -0.000519 -0.000536  ... -0.000196 -0.000143 -0.000464   \n",
       "3   -0.000546 -0.000668 -0.000664  ...  0.002124 -0.000161 -0.000466   \n",
       "4   -0.000512 -0.000648 -0.000658  ...  0.000506 -0.000228 -0.000492   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "935 -0.000087 -0.000252 -0.000330  ... -0.001759 -0.000263 -0.000510   \n",
       "936  0.000031  0.000013  0.000001  ... -0.002138 -0.000398 -0.000639   \n",
       "937 -0.000095 -0.000073 -0.000039  ... -0.002284 -0.000174 -0.000267   \n",
       "938 -0.000132 -0.000203 -0.000236  ... -0.001338 -0.000106 -0.000385   \n",
       "939 -0.000118 -0.000051  0.000011  ... -0.004223 -0.000174 -0.000530   \n",
       "\n",
       "         8264      8260      8256      8252      8248      8244      8240  \n",
       "0   -0.000147  0.000269  0.000194 -0.000101 -0.000236  0.000005  0.000254  \n",
       "1   -0.000191  0.000250  0.000220 -0.000099 -0.000252  0.000024  0.000300  \n",
       "2   -0.000232  0.000249  0.000254 -0.000132 -0.000293  0.000064  0.000383  \n",
       "3   -0.000230  0.000262  0.000252 -0.000165 -0.000313  0.000078  0.000407  \n",
       "4   -0.000171  0.000308  0.000224 -0.000214 -0.000324  0.000084  0.000405  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "935 -0.000133  0.000296  0.000145 -0.000253 -0.000316  0.000053  0.000334  \n",
       "936 -0.000079  0.000252 -0.000026 -0.000318 -0.000296 -0.000039  0.000152  \n",
       "937  0.000044  0.000294  0.000082 -0.000342 -0.000470 -0.000207  0.000016  \n",
       "938 -0.000189  0.000302  0.000252 -0.000234 -0.000348  0.000131  0.000491  \n",
       "939 -0.000254  0.000289  0.000285 -0.000118 -0.000233  0.000195  0.000542  \n",
       "\n",
       "[940 rows x 245 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leave-One-Group-Out CV Logistic Regression Model (Flipped) Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Non-Thai       0.92      0.88      0.90       470\n",
      "        Thai       0.89      0.93      0.91       470\n",
      "\n",
      "    accuracy                           0.90       940\n",
      "   macro avg       0.90      0.90      0.90       940\n",
      "weighted avg       0.90      0.90      0.90       940\n",
      "\n",
      "Confusion Matrix (Flipped):\n",
      "          Non-Thai  Thai\n",
      "Non-Thai       415    55\n",
      "Thai            35   435\n",
      "Class: Non-Thai\n",
      "precision: 0.9222222222222223\n",
      "recall: 0.8829787234042553\n",
      "f1-score: 0.9021739130434783\n",
      "support: 470.0\n",
      "\n",
      "Class: Thai\n",
      "precision: 0.8877551020408163\n",
      "recall: 0.925531914893617\n",
      "f1-score: 0.90625\n",
      "support: 470.0\n",
      "\n",
      "accuracy: 0.9042553191489362\n",
      "\n",
      "Class: macro avg\n",
      "precision: 0.9049886621315193\n",
      "recall: 0.9042553191489362\n",
      "f1-score: 0.9042119565217391\n",
      "support: 940.0\n",
      "\n",
      "Class: weighted avg\n",
      "precision: 0.9049886621315193\n",
      "recall: 0.9042553191489362\n",
      "f1-score: 0.9042119565217391\n",
      "support: 940.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "\n",
    "# Load the dataset\n",
    "file_path = r\"data/data file 2b/data_1_1_der.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Define the target variable and feature columns\n",
    "target_v00 = 'thnoth_name'\n",
    "features_v00 = data.columns.difference(['thnoth_name', 'prov_char'])\n",
    "X = data[features_v00]\n",
    "y = data[target_v00]\n",
    "groups = data['prov_char']\n",
    "\n",
    "# Encode the target variable\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "# Standardize the data (mean=0, variance=1)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Initialize Logistic Regression model with a different solver\n",
    "logreg_model = LogisticRegression(random_state=42, max_iter=10000, solver='liblinear')\n",
    "\n",
    "# Initialize Leave-One-Group-Out Cross-Validation\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "# Initialize lists to store results\n",
    "test_accuracies_logreg = []\n",
    "y_true_all_logreg = []\n",
    "y_pred_all_logreg = []\n",
    "\n",
    "# Apply LOGO-CV\n",
    "for train_index, test_index in logo.split(X_scaled, y_encoded, groups):\n",
    "    X_train, X_test = X_scaled[train_index], X_scaled[test_index]\n",
    "    y_train, y_test = y_encoded[train_index], y_encoded[test_index]\n",
    "    \n",
    "    # Fit the model\n",
    "    logreg_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict the test samples\n",
    "    y_test_pred = logreg_model.predict(X_test)\n",
    "    \n",
    "    # Calculate and store test accuracies\n",
    "    test_accuracies_logreg.append(accuracy_score(y_test, y_test_pred))\n",
    "    \n",
    "    # Store the prediction and actual value\n",
    "    y_true_all_logreg.extend(y_test)\n",
    "    y_pred_all_logreg.extend(y_test_pred)\n",
    "\n",
    "# Flip the predictions (numerically encoded)\n",
    "y_pred_lg_flipped = [1 - pred for pred in y_pred_all_logreg]\n",
    "\n",
    "# Generate the classification report for the flipped predictions\n",
    "report_logreg_flipped = classification_report(y_true_all_logreg, y_pred_lg_flipped, target_names=le.classes_, output_dict=True)\n",
    "print(\"Leave-One-Group-Out CV Logistic Regression Model (Flipped) Classification Report\")\n",
    "print(classification_report(y_true_all_logreg, y_pred_lg_flipped, target_names=le.classes_))\n",
    "\n",
    "# Generate and display the confusion matrix for the flipped predictions\n",
    "cm_logreg_flipped = confusion_matrix(y_true_all_logreg, y_pred_lg_flipped)\n",
    "df_cm_logreg_flipped = pd.DataFrame(cm_logreg_flipped, index=le.classes_, columns=le.classes_)\n",
    "print(\"Confusion Matrix (Flipped):\")\n",
    "print(df_cm_logreg_flipped)\n",
    "\n",
    "# Display the detailed results for the flipped predictions\n",
    "for class_name, metrics in report_logreg_flipped.items():\n",
    "    if isinstance(metrics, dict):\n",
    "        print(f\"Class: {class_name}\")\n",
    "        for metric_name, score in metrics.items():\n",
    "            print(f\"{metric_name}: {score}\")\n",
    "    else:\n",
    "        print(f\"{class_name}: {metrics}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leave-One-Group-Out CV - Average Test Accuracy (Reversed Classes): 0.8776595744680851\n",
      "Leave-One-Group-Out CV LDA Model Classification Report (Reversed Classes)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Non-Thai       0.90      0.85      0.87       470\n",
      "        Thai       0.86      0.90      0.88       470\n",
      "\n",
      "    accuracy                           0.88       940\n",
      "   macro avg       0.88      0.88      0.88       940\n",
      "weighted avg       0.88      0.88      0.88       940\n",
      "\n",
      "Confusion Matrix (Reversed Classes):\n",
      "          Non-Thai  Thai\n",
      "Non-Thai       400    70\n",
      "Thai            45   425\n",
      "Class: Non-Thai\n",
      "precision: 0.898876404494382\n",
      "recall: 0.851063829787234\n",
      "f1-score: 0.8743169398907104\n",
      "support: 470.0\n",
      "\n",
      "Class: Thai\n",
      "precision: 0.8585858585858586\n",
      "recall: 0.9042553191489362\n",
      "f1-score: 0.8808290155440415\n",
      "support: 470.0\n",
      "\n",
      "accuracy: 0.8776595744680851\n",
      "\n",
      "Class: macro avg\n",
      "precision: 0.8787311315401203\n",
      "recall: 0.8776595744680851\n",
      "f1-score: 0.877572977717376\n",
      "support: 940.0\n",
      "\n",
      "Class: weighted avg\n",
      "precision: 0.8787311315401202\n",
      "recall: 0.8776595744680851\n",
      "f1-score: 0.8775729777173759\n",
      "support: 940.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Initialize the LDA Classifier\n",
    "model_lda = LinearDiscriminantAnalysis()\n",
    "\n",
    "# Initialize Leave-One-Group-Out Cross-Validation\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "# Initialize lists to store results\n",
    "accuracies_lda = []\n",
    "y_true_all_lda = []\n",
    "y_pred_all_lda = []\n",
    "\n",
    "# Apply LOGO-CV\n",
    "for train_index, test_index in logo.split(X, y_encoded, groups):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y_encoded[train_index], y_encoded[test_index]\n",
    "    \n",
    "    # Fit the model\n",
    "    model_lda.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict the test samples\n",
    "    y_test_pred = model_lda.predict(X_test)\n",
    "    \n",
    "    # Calculate and store the accuracy\n",
    "    accuracies_lda.append(accuracy_score(y_test, y_test_pred))\n",
    "    \n",
    "    # Store the prediction and actual values\n",
    "    y_true_all_lda.extend(y_test)\n",
    "    y_pred_all_lda.extend(y_test_pred)\n",
    "\n",
    "# Reverse the predicted classes (numerically encoded)\n",
    "y_pred_all_lda_reversed = [1 - y for y in y_pred_all_lda]\n",
    "\n",
    "# Calculate the overall accuracy for the reversed predictions\n",
    "overall_accuracy_lda_reversed = accuracy_score(y_true_all_lda, y_pred_all_lda_reversed)\n",
    "\n",
    "print(f'Leave-One-Group-Out CV - Average Test Accuracy (Reversed Classes): {overall_accuracy_lda_reversed}')\n",
    "print(\"Leave-One-Group-Out CV LDA Model Classification Report (Reversed Classes)\")\n",
    "report_lda_logo_reversed = classification_report(y_true_all_lda, y_pred_all_lda_reversed, target_names=le.classes_, output_dict=True)\n",
    "print(classification_report(y_true_all_lda, y_pred_all_lda_reversed, target_names=le.classes_))\n",
    "\n",
    "# Generate and display the confusion matrix for the reversed predictions\n",
    "cm_logo_lda_reversed = confusion_matrix(y_true_all_lda, y_pred_all_lda_reversed)\n",
    "df_cm_lda_reversed = pd.DataFrame(cm_logo_lda_reversed, index=le.classes_, columns=le.classes_)\n",
    "print(\"Confusion Matrix (Reversed Classes):\")\n",
    "print(df_cm_lda_reversed)\n",
    "\n",
    "# Display the detailed results for the reversed predictions\n",
    "for class_name, metrics in report_lda_logo_reversed.items():\n",
    "    if isinstance(metrics, dict):\n",
    "        print(f\"Class: {class_name}\")\n",
    "        for metric_name, score in metrics.items():\n",
    "            print(f\"{metric_name}: {score}\")\n",
    "    else:\n",
    "        print(f\"{class_name}: {metrics}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leave-One-Group-Out CV - Overall Test Accuracy: 0.8414893617021276\n",
      "Leave-One-Group-Out CV SVM Model Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Non-Thai       0.93      0.74      0.82       470\n",
      "        Thai       0.78      0.94      0.86       470\n",
      "\n",
      "    accuracy                           0.84       940\n",
      "   macro avg       0.86      0.84      0.84       940\n",
      "weighted avg       0.86      0.84      0.84       940\n",
      "\n",
      "Confusion Matrix:\n",
      "          Non-Thai  Thai\n",
      "Non-Thai       347   123\n",
      "Thai            26   444\n",
      "Class: Non-Thai\n",
      "precision: 0.9302949061662198\n",
      "recall: 0.7382978723404255\n",
      "f1-score: 0.8232502965599051\n",
      "support: 470.0\n",
      "\n",
      "Class: Thai\n",
      "precision: 0.783068783068783\n",
      "recall: 0.9446808510638298\n",
      "f1-score: 0.8563162970106075\n",
      "support: 470.0\n",
      "\n",
      "accuracy: 0.8414893617021276\n",
      "\n",
      "Class: macro avg\n",
      "precision: 0.8566818446175014\n",
      "recall: 0.8414893617021277\n",
      "f1-score: 0.8397832967852563\n",
      "support: 940.0\n",
      "\n",
      "Class: weighted avg\n",
      "precision: 0.8566818446175014\n",
      "recall: 0.8414893617021276\n",
      "f1-score: 0.8397832967852563\n",
      "support: 940.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled_svm = scaler.fit_transform(X)\n",
    "\n",
    "# Initialize the SVM classifier\n",
    "svm_model = SVC(kernel='rbf', random_state=42)\n",
    "\n",
    "# Initialize LOGO-CV\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "# Initialize lists to store results\n",
    "y_true_svm_all = []\n",
    "y_pred_svm_all = []\n",
    "\n",
    "# Perform LOGO-CV\n",
    "for train_index, test_index in logo.split(X_scaled_svm, y_encoded, groups):\n",
    "    X_train, X_test = X_scaled_svm[train_index], X_scaled_svm[test_index]\n",
    "    y_train, y_test = y_encoded[train_index], y_encoded[test_index]\n",
    "    \n",
    "    # Fit the model\n",
    "    svm_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict the test samples\n",
    "    y_test_pred = svm_model.predict(X_test)\n",
    "    \n",
    "    # Store the prediction and actual value\n",
    "    y_true_svm_all.extend(y_test)\n",
    "    y_pred_svm_all.extend(y_test_pred)\n",
    "\n",
    "# Calculate the overall accuracy\n",
    "overall_accuracy_svm = accuracy_score(y_true_svm_all, y_pred_svm_all)\n",
    "\n",
    "print(f'Leave-One-Group-Out CV - Overall Test Accuracy: {overall_accuracy_svm}')\n",
    "\n",
    "# Generate the classification report for the overall test predictions\n",
    "report_svm_logo = classification_report(y_true_svm_all, y_pred_svm_all, target_names=le.classes_, output_dict=True)\n",
    "print(\"Leave-One-Group-Out CV SVM Model Classification Report\")\n",
    "print(classification_report(y_true_svm_all, y_pred_svm_all, target_names=le.classes_))\n",
    "\n",
    "# Print the confusion matrix in text format\n",
    "conf_matrix_svm = confusion_matrix(y_true_svm_all, y_pred_svm_all)\n",
    "conf_matrix_df_svm = pd.DataFrame(conf_matrix_svm, index=le.classes_, columns=le.classes_)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix_df_svm)\n",
    "\n",
    "# Print the detailed results\n",
    "for class_name, metrics in report_svm_logo.items():\n",
    "    if isinstance(metrics, dict):\n",
    "        print(f\"Class: {class_name}\")\n",
    "        for metric_name, score in metrics.items():\n",
    "            print(f\"{metric_name}: {score}\")\n",
    "    else:\n",
    "        print(f\"{class_name}: {metrics}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leave-One-Group-Out CV Logistic Regression Model (Flipped) Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Non-Thai       0.65      0.74      0.69       470\n",
      "        Thai       0.70      0.60      0.65       470\n",
      "\n",
      "    accuracy                           0.67       940\n",
      "   macro avg       0.67      0.67      0.67       940\n",
      "weighted avg       0.67      0.67      0.67       940\n",
      "\n",
      "Confusion Matrix (Flipped):\n",
      "          Non-Thai  Thai\n",
      "Non-Thai       348   122\n",
      "Thai           188   282\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# Create polynomial features\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_poly = poly.fit_transform(X)\n",
    "\n",
    "# Perform Recursive Feature Elimination\n",
    "logreg = LogisticRegression(random_state=42, max_iter=10000, solver='liblinear')\n",
    "selector = RFE(logreg, n_features_to_select=10, step=1)\n",
    "selector = selector.fit(X_poly, y_encoded)\n",
    "\n",
    "# Get selected features\n",
    "X_selected = selector.transform(X_poly)\n",
    "\n",
    "# Standardize the selected features\n",
    "scaler = StandardScaler()\n",
    "X_selected_scaled = scaler.fit_transform(X_selected)\n",
    "\n",
    "# Apply LOGO-CV with the selected features\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "test_accuracies_logreg = []\n",
    "y_true_all_logreg = []\n",
    "y_pred_all_logreg = []\n",
    "\n",
    "for train_index, test_index in logo.split(X_selected_scaled, y_encoded, groups):\n",
    "    X_train, X_test = X_selected_scaled[train_index], X_selected_scaled[test_index]\n",
    "    y_train, y_test = y_encoded[train_index], y_encoded[test_index]\n",
    "    \n",
    "    logreg.fit(X_train, y_train)\n",
    "    y_test_pred = logreg.predict(X_test)\n",
    "    \n",
    "    test_accuracies_logreg.append(accuracy_score(y_test, y_test_pred))\n",
    "    y_true_all_logreg.extend(y_test)\n",
    "    y_pred_all_logreg.extend(y_test_pred)\n",
    "\n",
    "y_pred_lg_flipped = [1 - pred for pred in y_pred_all_logreg]\n",
    "\n",
    "report_logreg_flipped = classification_report(y_true_all_logreg, y_pred_lg_flipped, target_names=le.classes_, output_dict=True)\n",
    "print(\"Leave-One-Group-Out CV Logistic Regression Model (Flipped) Classification Report\")\n",
    "print(classification_report(y_true_all_logreg, y_pred_lg_flipped, target_names=le.classes_))\n",
    "\n",
    "cm_logreg_flipped = confusion_matrix(y_true_all_logreg, y_pred_lg_flipped)\n",
    "df_cm_logreg_flipped = pd.DataFrame(cm_logreg_flipped, index=le.classes_, columns=le.classes_)\n",
    "print(\"Confusion Matrix (Flipped):\")\n",
    "print(df_cm_logreg_flipped)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:  {'C': 0.1, 'solver': 'saga'}\n",
      "Leave-One-Group-Out CV Logistic Regression Model (Flipped) Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Non-Thai       0.71      0.78      0.75       470\n",
      "        Thai       0.76      0.69      0.72       470\n",
      "\n",
      "    accuracy                           0.73       940\n",
      "   macro avg       0.74      0.73      0.73       940\n",
      "weighted avg       0.74      0.73      0.73       940\n",
      "\n",
      "Confusion Matrix (Flipped):\n",
      "          Non-Thai  Thai\n",
      "Non-Thai       367   103\n",
      "Thai           148   322\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],\n",
    "    'solver': ['liblinear', 'saga', 'newton-cg', 'lbfgs', 'sag']\n",
    "}\n",
    "\n",
    "# Initialize Grid Search\n",
    "grid_search = GridSearchCV(LogisticRegression(random_state=42, max_iter=10000), param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Fit Grid Search\n",
    "grid_search.fit(X_selected_scaled, y_encoded)\n",
    "\n",
    "# Best parameters\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "\n",
    "# Apply LOGO-CV with the best parameters\n",
    "best_logreg = grid_search.best_estimator_\n",
    "\n",
    "test_accuracies_logreg = []\n",
    "y_true_all_logreg = []\n",
    "y_pred_all_logreg = []\n",
    "\n",
    "for train_index, test_index in logo.split(X_selected_scaled, y_encoded, groups):\n",
    "    X_train, X_test = X_selected_scaled[train_index], X_selected_scaled[test_index]\n",
    "    y_train, y_test = y_encoded[train_index], y_encoded[test_index]\n",
    "    \n",
    "    best_logreg.fit(X_train, y_train)\n",
    "    y_test_pred = best_logreg.predict(X_test)\n",
    "    \n",
    "    test_accuracies_logreg.append(accuracy_score(y_test, y_test_pred))\n",
    "    y_true_all_logreg.extend(y_test)\n",
    "    y_pred_all_logreg.extend(y_test_pred)\n",
    "\n",
    "y_pred_lg_flipped = [1 - pred for pred in y_pred_all_logreg]\n",
    "\n",
    "report_logreg_flipped = classification_report(y_true_all_logreg, y_pred_lg_flipped, target_names=le.classes_, output_dict=True)\n",
    "print(\"Leave-One-Group-Out CV Logistic Regression Model (Flipped) Classification Report\")\n",
    "print(classification_report(y_true_all_logreg, y_pred_lg_flipped, target_names=le.classes_))\n",
    "\n",
    "cm_logreg_flipped = confusion_matrix(y_true_all_logreg, y_pred_lg_flipped)\n",
    "df_cm_logreg_flipped = pd.DataFrame(cm_logreg_flipped, index=le.classes_, columns=le.classes_)\n",
    "print(\"Confusion Matrix (Flipped):\")\n",
    "print(df_cm_logreg_flipped)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
