{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df_0 = pd.read_csv(r\"C:\\Users\\pingk\\Downloads\\fadhli nitip\\NIR_3g1_clnd_blncd.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prov_char</th>\n",
       "      <th>sample_code</th>\n",
       "      <th>11544</th>\n",
       "      <th>11540</th>\n",
       "      <th>11536</th>\n",
       "      <th>11532</th>\n",
       "      <th>11528</th>\n",
       "      <th>11524</th>\n",
       "      <th>11520</th>\n",
       "      <th>11516</th>\n",
       "      <th>...</th>\n",
       "      <th>3968</th>\n",
       "      <th>3964</th>\n",
       "      <th>3960</th>\n",
       "      <th>3956</th>\n",
       "      <th>3952</th>\n",
       "      <th>tgp_name</th>\n",
       "      <th>dgp_name</th>\n",
       "      <th>fgp_name</th>\n",
       "      <th>country_name</th>\n",
       "      <th>thnoth_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CBI</td>\n",
       "      <td>TH-CBI-028-2312-001</td>\n",
       "      <td>0.632215</td>\n",
       "      <td>0.632145</td>\n",
       "      <td>0.631933</td>\n",
       "      <td>0.631887</td>\n",
       "      <td>0.632053</td>\n",
       "      <td>0.632104</td>\n",
       "      <td>0.631949</td>\n",
       "      <td>0.631858</td>\n",
       "      <td>...</td>\n",
       "      <td>3.920535</td>\n",
       "      <td>3.933646</td>\n",
       "      <td>3.939754</td>\n",
       "      <td>3.925399</td>\n",
       "      <td>3.855970</td>\n",
       "      <td>Group 1</td>\n",
       "      <td>Group 1</td>\n",
       "      <td>Group 1</td>\n",
       "      <td>Thailand</td>\n",
       "      <td>Thai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CBI</td>\n",
       "      <td>TH-CBI-028-2312-001</td>\n",
       "      <td>0.327512</td>\n",
       "      <td>0.327577</td>\n",
       "      <td>0.327569</td>\n",
       "      <td>0.327574</td>\n",
       "      <td>0.327606</td>\n",
       "      <td>0.327647</td>\n",
       "      <td>0.327661</td>\n",
       "      <td>0.327669</td>\n",
       "      <td>...</td>\n",
       "      <td>3.696666</td>\n",
       "      <td>3.666743</td>\n",
       "      <td>3.634470</td>\n",
       "      <td>3.617612</td>\n",
       "      <td>3.602382</td>\n",
       "      <td>Group 1</td>\n",
       "      <td>Group 1</td>\n",
       "      <td>Group 1</td>\n",
       "      <td>Thailand</td>\n",
       "      <td>Thai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CBI</td>\n",
       "      <td>TH-CBI-028-2312-001</td>\n",
       "      <td>0.409853</td>\n",
       "      <td>0.409919</td>\n",
       "      <td>0.410024</td>\n",
       "      <td>0.410191</td>\n",
       "      <td>0.410317</td>\n",
       "      <td>0.410413</td>\n",
       "      <td>0.410444</td>\n",
       "      <td>0.410356</td>\n",
       "      <td>...</td>\n",
       "      <td>3.774406</td>\n",
       "      <td>3.756090</td>\n",
       "      <td>3.743530</td>\n",
       "      <td>3.741426</td>\n",
       "      <td>3.725178</td>\n",
       "      <td>Group 1</td>\n",
       "      <td>Group 1</td>\n",
       "      <td>Group 1</td>\n",
       "      <td>Thailand</td>\n",
       "      <td>Thai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CBI</td>\n",
       "      <td>TH-CBI-028-2312-001</td>\n",
       "      <td>0.325830</td>\n",
       "      <td>0.325964</td>\n",
       "      <td>0.326084</td>\n",
       "      <td>0.326087</td>\n",
       "      <td>0.326056</td>\n",
       "      <td>0.326163</td>\n",
       "      <td>0.326322</td>\n",
       "      <td>0.326451</td>\n",
       "      <td>...</td>\n",
       "      <td>3.705976</td>\n",
       "      <td>3.696601</td>\n",
       "      <td>3.691616</td>\n",
       "      <td>3.661514</td>\n",
       "      <td>3.621867</td>\n",
       "      <td>Group 1</td>\n",
       "      <td>Group 1</td>\n",
       "      <td>Group 1</td>\n",
       "      <td>Thailand</td>\n",
       "      <td>Thai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CBI</td>\n",
       "      <td>TH-CBI-028-2312-001</td>\n",
       "      <td>-0.061811</td>\n",
       "      <td>-0.061787</td>\n",
       "      <td>-0.061711</td>\n",
       "      <td>-0.061622</td>\n",
       "      <td>-0.061589</td>\n",
       "      <td>-0.061598</td>\n",
       "      <td>-0.061591</td>\n",
       "      <td>-0.061547</td>\n",
       "      <td>...</td>\n",
       "      <td>3.634053</td>\n",
       "      <td>3.624039</td>\n",
       "      <td>3.576779</td>\n",
       "      <td>3.535356</td>\n",
       "      <td>3.525223</td>\n",
       "      <td>Group 1</td>\n",
       "      <td>Group 1</td>\n",
       "      <td>Group 1</td>\n",
       "      <td>Thailand</td>\n",
       "      <td>Thai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>KBX</td>\n",
       "      <td>ID-KBX-068-2306-014</td>\n",
       "      <td>0.178972</td>\n",
       "      <td>0.178900</td>\n",
       "      <td>0.178732</td>\n",
       "      <td>0.178545</td>\n",
       "      <td>0.178437</td>\n",
       "      <td>0.178507</td>\n",
       "      <td>0.178636</td>\n",
       "      <td>0.178680</td>\n",
       "      <td>...</td>\n",
       "      <td>3.901559</td>\n",
       "      <td>3.911542</td>\n",
       "      <td>3.876838</td>\n",
       "      <td>3.833573</td>\n",
       "      <td>3.823722</td>\n",
       "      <td>Group 3</td>\n",
       "      <td>Group 2</td>\n",
       "      <td>Group 4</td>\n",
       "      <td>Indonesia</td>\n",
       "      <td>Non-Thai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>KBX</td>\n",
       "      <td>ID-KBX-068-2306-013</td>\n",
       "      <td>-0.110432</td>\n",
       "      <td>-0.110301</td>\n",
       "      <td>-0.110232</td>\n",
       "      <td>-0.110244</td>\n",
       "      <td>-0.110226</td>\n",
       "      <td>-0.110072</td>\n",
       "      <td>-0.109857</td>\n",
       "      <td>-0.109721</td>\n",
       "      <td>...</td>\n",
       "      <td>3.660768</td>\n",
       "      <td>3.673353</td>\n",
       "      <td>3.698740</td>\n",
       "      <td>3.695536</td>\n",
       "      <td>3.653846</td>\n",
       "      <td>Group 3</td>\n",
       "      <td>Group 2</td>\n",
       "      <td>Group 4</td>\n",
       "      <td>Indonesia</td>\n",
       "      <td>Non-Thai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>KBX</td>\n",
       "      <td>ID-KBX-068-2306-005</td>\n",
       "      <td>-0.131264</td>\n",
       "      <td>-0.131238</td>\n",
       "      <td>-0.131196</td>\n",
       "      <td>-0.131155</td>\n",
       "      <td>-0.131105</td>\n",
       "      <td>-0.131080</td>\n",
       "      <td>-0.131078</td>\n",
       "      <td>-0.131065</td>\n",
       "      <td>...</td>\n",
       "      <td>3.548929</td>\n",
       "      <td>3.524792</td>\n",
       "      <td>3.501194</td>\n",
       "      <td>3.485311</td>\n",
       "      <td>3.449790</td>\n",
       "      <td>Group 3</td>\n",
       "      <td>Group 2</td>\n",
       "      <td>Group 4</td>\n",
       "      <td>Indonesia</td>\n",
       "      <td>Non-Thai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>NWT</td>\n",
       "      <td>TH-NWT-047-2311-001</td>\n",
       "      <td>0.781770</td>\n",
       "      <td>0.781588</td>\n",
       "      <td>0.781292</td>\n",
       "      <td>0.781034</td>\n",
       "      <td>0.780967</td>\n",
       "      <td>0.781121</td>\n",
       "      <td>0.781453</td>\n",
       "      <td>0.781679</td>\n",
       "      <td>...</td>\n",
       "      <td>3.844008</td>\n",
       "      <td>3.814409</td>\n",
       "      <td>3.816815</td>\n",
       "      <td>3.837593</td>\n",
       "      <td>3.818724</td>\n",
       "      <td>Group 3</td>\n",
       "      <td>Group 2</td>\n",
       "      <td>Group 3</td>\n",
       "      <td>Thailand</td>\n",
       "      <td>Thai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>BBX</td>\n",
       "      <td>ID-BBX-095-2401-004</td>\n",
       "      <td>-0.137837</td>\n",
       "      <td>-0.137824</td>\n",
       "      <td>-0.137810</td>\n",
       "      <td>-0.137743</td>\n",
       "      <td>-0.137608</td>\n",
       "      <td>-0.137484</td>\n",
       "      <td>-0.137441</td>\n",
       "      <td>-0.137415</td>\n",
       "      <td>...</td>\n",
       "      <td>3.507692</td>\n",
       "      <td>3.490361</td>\n",
       "      <td>3.468226</td>\n",
       "      <td>3.447421</td>\n",
       "      <td>3.407099</td>\n",
       "      <td>Group 3</td>\n",
       "      <td>Group 2</td>\n",
       "      <td>Group 3</td>\n",
       "      <td>Indonesia</td>\n",
       "      <td>Non-Thai</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows × 1906 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    prov_char          sample_code     11544     11540     11536     11532  \\\n",
       "0         CBI  TH-CBI-028-2312-001  0.632215  0.632145  0.631933  0.631887   \n",
       "1         CBI  TH-CBI-028-2312-001  0.327512  0.327577  0.327569  0.327574   \n",
       "2         CBI  TH-CBI-028-2312-001  0.409853  0.409919  0.410024  0.410191   \n",
       "3         CBI  TH-CBI-028-2312-001  0.325830  0.325964  0.326084  0.326087   \n",
       "4         CBI  TH-CBI-028-2312-001 -0.061811 -0.061787 -0.061711 -0.061622   \n",
       "..        ...                  ...       ...       ...       ...       ...   \n",
       "115       KBX  ID-KBX-068-2306-014  0.178972  0.178900  0.178732  0.178545   \n",
       "116       KBX  ID-KBX-068-2306-013 -0.110432 -0.110301 -0.110232 -0.110244   \n",
       "117       KBX  ID-KBX-068-2306-005 -0.131264 -0.131238 -0.131196 -0.131155   \n",
       "118       NWT  TH-NWT-047-2311-001  0.781770  0.781588  0.781292  0.781034   \n",
       "119       BBX  ID-BBX-095-2401-004 -0.137837 -0.137824 -0.137810 -0.137743   \n",
       "\n",
       "        11528     11524     11520     11516  ...      3968      3964  \\\n",
       "0    0.632053  0.632104  0.631949  0.631858  ...  3.920535  3.933646   \n",
       "1    0.327606  0.327647  0.327661  0.327669  ...  3.696666  3.666743   \n",
       "2    0.410317  0.410413  0.410444  0.410356  ...  3.774406  3.756090   \n",
       "3    0.326056  0.326163  0.326322  0.326451  ...  3.705976  3.696601   \n",
       "4   -0.061589 -0.061598 -0.061591 -0.061547  ...  3.634053  3.624039   \n",
       "..        ...       ...       ...       ...  ...       ...       ...   \n",
       "115  0.178437  0.178507  0.178636  0.178680  ...  3.901559  3.911542   \n",
       "116 -0.110226 -0.110072 -0.109857 -0.109721  ...  3.660768  3.673353   \n",
       "117 -0.131105 -0.131080 -0.131078 -0.131065  ...  3.548929  3.524792   \n",
       "118  0.780967  0.781121  0.781453  0.781679  ...  3.844008  3.814409   \n",
       "119 -0.137608 -0.137484 -0.137441 -0.137415  ...  3.507692  3.490361   \n",
       "\n",
       "         3960      3956      3952  tgp_name  dgp_name  fgp_name  country_name  \\\n",
       "0    3.939754  3.925399  3.855970   Group 1   Group 1   Group 1      Thailand   \n",
       "1    3.634470  3.617612  3.602382   Group 1   Group 1   Group 1      Thailand   \n",
       "2    3.743530  3.741426  3.725178   Group 1   Group 1   Group 1      Thailand   \n",
       "3    3.691616  3.661514  3.621867   Group 1   Group 1   Group 1      Thailand   \n",
       "4    3.576779  3.535356  3.525223   Group 1   Group 1   Group 1      Thailand   \n",
       "..        ...       ...       ...       ...       ...       ...           ...   \n",
       "115  3.876838  3.833573  3.823722   Group 3   Group 2   Group 4     Indonesia   \n",
       "116  3.698740  3.695536  3.653846   Group 3   Group 2   Group 4     Indonesia   \n",
       "117  3.501194  3.485311  3.449790   Group 3   Group 2   Group 4     Indonesia   \n",
       "118  3.816815  3.837593  3.818724   Group 3   Group 2   Group 3      Thailand   \n",
       "119  3.468226  3.447421  3.407099   Group 3   Group 2   Group 3     Indonesia   \n",
       "\n",
       "     thnoth_name  \n",
       "0           Thai  \n",
       "1           Thai  \n",
       "2           Thai  \n",
       "3           Thai  \n",
       "4           Thai  \n",
       "..           ...  \n",
       "115     Non-Thai  \n",
       "116     Non-Thai  \n",
       "117     Non-Thai  \n",
       "118         Thai  \n",
       "119     Non-Thai  \n",
       "\n",
       "[120 rows x 1906 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting Regions of Interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the regions of interest\n",
    "regions_of_interest = [\n",
    "    (4641, 4681), (4867, 5510), (5657, 5826), (7057, 7097),\n",
    "    (7169, 7209), (8238, 8278)\n",
    "]\n",
    "\n",
    "# Extract columns corresponding to the regions of interest\n",
    "columns_to_focus = []\n",
    "for start, end in regions_of_interest:\n",
    "    columns_to_focus.extend([col for col in df_0.columns[4:-5] if start <= float(col) <= end])\n",
    "\n",
    "# Create a new DataFrame with the selected regions\n",
    "df_0_selected_regions = df_0[columns_to_focus]\n",
    "\n",
    "# Combine the selected regions with the target column and other relevant columns\n",
    "df_0_selected_regions = pd.concat([df_0[['country_name', 'prov_char']], df_0_selected_regions], axis=1)\n",
    "\n",
    "# Save the DataFrame for further processing\n",
    "df_0_selected_regions.to_csv('data/data file 3/data_1.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FUNCTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import savgol_filter\n",
    "\n",
    "# Function for baseline correction with dynamic window length\n",
    "def baseline_correction(spectrum, default_window_length=15, polyorder=3):\n",
    "    spectrum_length = len(spectrum)\n",
    "    if spectrum_length < default_window_length:\n",
    "        window_length = spectrum_length // 2 * 2 + 1  # Make window length odd and less than the size of the spectrum\n",
    "    else:\n",
    "        window_length = default_window_length\n",
    "    baseline = savgol_filter(spectrum, window_length, polyorder, mode='nearest')\n",
    "    corrected_spectrum = spectrum - baseline\n",
    "    return corrected_spectrum\n",
    "\n",
    "# Apply baseline correction\n",
    "df_baseline_corrected_v0 = df_0_selected_regions.copy()\n",
    "for col in columns_to_focus:\n",
    "    df_baseline_corrected_v0[col] = baseline_correction(df_baseline_corrected_v0[col])\n",
    "\n",
    "# Save the baseline corrected data\n",
    "df_baseline_corrected_v0.to_csv('data/data file 3/data_1_bslcrct.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SavGol Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for Savitzky-Golay smoothing\n",
    "def savitzky_golay_smoothing(spectrum, default_window_length=11, polyorder=2):\n",
    "    window_length = min(default_window_length, len(spectrum) // 2 * 2 + 1)  # Make window length odd and less than or equal to the size of the spectrum\n",
    "    if window_length < 3:  # Ensure window length is at least 3\n",
    "        window_length = 3\n",
    "    return savgol_filter(spectrum, window_length, polyorder, mode='nearest')  # Set mode to 'nearest'\n",
    "\n",
    "# Apply smoothing\n",
    "df_smoothed_v0 = df_baseline_corrected_v0.copy()\n",
    "for col in columns_to_focus:\n",
    "    df_smoothed_v0[col] = savitzky_golay_smoothing(df_smoothed_v0[col])\n",
    "\n",
    "# Save the smoothed data\n",
    "df_smoothed_v0.to_csv('data/data file 3/data_1_smoothed.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for normalization (Min-Max scaling)\n",
    "def min_max_normalization(spectrum):\n",
    "    return (spectrum - np.min(spectrum)) / (np.max(spectrum) - np.min(spectrum))\n",
    "\n",
    "# Apply normalization\n",
    "df_normalized_v0 = df_smoothed_v0.copy()\n",
    "for col in columns_to_focus:\n",
    "    df_normalized_v0[col] = min_max_normalization(df_normalized_v0[col])\n",
    "\n",
    "# Save the normalized data\n",
    "df_normalized_v0.to_csv('data/data file 3/data_1_normalized.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derivatization (np.gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the first derivative using np.gradient\n",
    "data_spectrum = df_normalized_v0.iloc[:, 2:].values\n",
    "first_derivative_np = np.gradient(data_spectrum, axis=1)\n",
    "\n",
    "# Calculate the second derivative using np.gradient\n",
    "second_derivative_np = np.gradient(first_derivative_np, axis=1)\n",
    "\n",
    "# Convert the results back to DataFrame\n",
    "data_1_der_np = pd.DataFrame(first_derivative_np, columns=df_normalized_v0.columns[2:])\n",
    "data_2_der_np = pd.DataFrame(second_derivative_np, columns=df_normalized_v0.columns[2:])\n",
    "\n",
    "# Combine the first two columns from the original dataset with the np.gradient derivatives\n",
    "\n",
    "# Extract the first two columns\n",
    "first_two_columns = df_normalized_v0.iloc[:, :2]\n",
    "\n",
    "# Combine the first two columns with the derivatives\n",
    "data_1_der_combined = pd.concat([first_two_columns, data_1_der_np], axis=1)\n",
    "data_2_der_combined = pd.concat([first_two_columns, data_2_der_np], axis=1)\n",
    "\n",
    "# Export the combined data to CSV\n",
    "data_1_der_combined.to_csv('data/data file 3/data_1_1_der.csv', index=False)\n",
    "data_2_der_combined.to_csv('data/data file 3/data_1_2_der.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derivatization (SavGol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the spectrum data\n",
    "data_spectrum = df_normalized_v0.iloc[:, 2:].values\n",
    "\n",
    "# Apply Savitzky-Golay filter for the first derivative\n",
    "first_derivative_savgol = savgol_filter(data_spectrum, window_length=5, polyorder=2, deriv=1, axis=1)\n",
    "\n",
    "# Apply Savitzky-Golay filter for the second derivative\n",
    "second_derivative_savgol = savgol_filter(data_spectrum, window_length=5, polyorder=2, deriv=2, axis=1)\n",
    "\n",
    "# Convert the results back to DataFrame\n",
    "data_1_der_savgol = pd.DataFrame(first_derivative_savgol, columns=df_normalized_v0.columns[2:])\n",
    "data_2_der_savgol = pd.DataFrame(second_derivative_savgol, columns=df_normalized_v0.columns[2:])\n",
    "\n",
    "# Extract the first two columns\n",
    "first_two_columns = df_normalized_v0.iloc[:, :2]\n",
    "\n",
    "# Combine the first two columns with the Savitzky-Golay derivatives\n",
    "data_1_der_savgol_combined = pd.concat([first_two_columns, data_1_der_savgol], axis=1)\n",
    "data_2_der_savgol_combined = pd.concat([first_two_columns, data_2_der_savgol], axis=1)\n",
    "\n",
    "# Export the combined data to CSV\n",
    "data_1_der_savgol_combined.to_csv('data/data file 3/data_1_1_der_savgol.csv', index=False)\n",
    "data_2_der_savgol_combined.to_csv('data/data file 3/data_1_2_der_savgol.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SNV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def snv(spectrum):\n",
    "    return (spectrum - np.mean(spectrum)) / np.std(spectrum)\n",
    "\n",
    "# Apply SNV to the selected regions\n",
    "df_snv = df_0_selected_regions.copy()\n",
    "for col in columns_to_focus:\n",
    "    df_snv[col] = snv(df_snv[col])\n",
    "\n",
    "# Save the SNV data\n",
    "df_snv.to_csv('data/data file 3/data_1_snv.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Normal Variate (RNV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnv(spectrum):\n",
    "    random_noise = np.random.normal(0, np.std(spectrum), spectrum.shape)\n",
    "    return spectrum + random_noise\n",
    "\n",
    "# Apply RNV to the selected regions\n",
    "df_rnv = df_0_selected_regions.copy()\n",
    "for col in columns_to_focus:\n",
    "    df_rnv[col] = rnv(df_rnv[col])\n",
    "\n",
    "# Save the RNV data\n",
    "df_rnv.to_csv('data/data file 3/data_1_rnv.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiplicative Scatter Correction (MSC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def msc(spectrum, reference):\n",
    "#     mean_spectrum = np.mean(reference, axis=0)\n",
    "#     fit = np.polyfit(mean_spectrum, spectrum, 1, full=True)\n",
    "#     corrected_spectrum = (spectrum - fit[0][1]) / fit[0][0]\n",
    "#     return corrected_spectrum\n",
    "\n",
    "# # Apply MSC to the selected regions\n",
    "# df_msc = df_0_selected_regions.copy()\n",
    "# for col in columns_to_focus:\n",
    "#     df_msc[col] = msc(df_msc[col])\n",
    "\n",
    "# # Save the MSC data\n",
    "# df_msc.to_csv('data/data file 3/data_1_msc2.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiplicative Scatter Correction (MSC) function\n",
    "def msc(input_data):\n",
    "    # Mean center the data\n",
    "    mean_spectrum = np.mean(input_data, axis=0)\n",
    "    input_data_centered = input_data - mean_spectrum\n",
    "\n",
    "    # Perform MSC\n",
    "    reference = np.mean(input_data, axis=0)\n",
    "    msc_data = np.zeros_like(input_data)\n",
    "\n",
    "    for i in range(input_data.shape[0]):\n",
    "        fit = np.polyfit(reference, input_data_centered[i, :], 1, full=True)\n",
    "        msc_data[i, :] = (input_data_centered[i, :] - fit[0][1]) / fit[0][0]\n",
    "    \n",
    "    return msc_data\n",
    "\n",
    "# Extract the spectral data from the dataframe\n",
    "spectral_data = df_0_selected_regions.iloc[:, 2:].values\n",
    "\n",
    "# Apply MSC\n",
    "msc_corrected_data = msc(spectral_data)\n",
    "\n",
    "# Create a new dataframe with the MSC corrected data\n",
    "msc_df = pd.DataFrame(msc_corrected_data, columns=df_0_selected_regions.columns[2:])\n",
    "msc_df.insert(0, 'prov_char', df_0_selected_regions['prov_char'])\n",
    "msc_df.insert(0, 'country_name', df_0_selected_regions['country_name'])\n",
    "\n",
    "# Save the MSC corrected data to a new CSV file\n",
    "msc_corrected_file_path = 'data/data file 3/data_1_msc_corrected.csv'\n",
    "msc_df.to_csv(msc_corrected_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification and Evaluation (40-fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target variable\n",
    "target = 'country_name'\n",
    "\n",
    "# Ensure columns_to_focus are correctly identified\n",
    "numeric_cols_df_0 = df_0_selected_regions.select_dtypes(include=[np.number]).columns.tolist()\n",
    "columns_to_focus = numeric_cols_df_0  # Ensure columns are correctly selected\n",
    "\n",
    "# Classification and evaluation function using 40-fold CV\n",
    "def classify_and_evaluate(df, columns):\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "    # Encode target variable\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(df[target])\n",
    "\n",
    "    # Define features\n",
    "    X = df[columns]\n",
    "\n",
    "    # Initialize the classifier\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "    # Perform cross-validation\n",
    "    scores = cross_val_score(model, X, y, cv=40)\n",
    "\n",
    "    # Print the results\n",
    "    print(f'Cross-Validation Accuracy: {np.mean(scores)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for No Preprocessing:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pingk\\OneDrive - Chulalongkorn University\\Documents\\mekargit\\cpocluster\\env\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=40.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Accuracy: 0.7\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluate each preprocessing method\n",
    "print(\"Evaluation for No Preprocessing:\")\n",
    "classify_and_evaluate(df_0_selected_regions, columns_to_focus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for Baseline Correction:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pingk\\OneDrive - Chulalongkorn University\\Documents\\mekargit\\cpocluster\\env\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=40.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Accuracy: 0.625\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluation for Baseline Correction:\")\n",
    "df_baseline_corrected_v0z = pd.read_csv('data/data file 3/data_1_bslcrct.csv')\n",
    "classify_and_evaluate(df_baseline_corrected_v0z, columns_to_focus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for Smoothing:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pingk\\OneDrive - Chulalongkorn University\\Documents\\mekargit\\cpocluster\\env\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=40.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Accuracy: 0.6833333333333333\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluation for Smoothing:\")\n",
    "df_smoothed_v0z = pd.read_csv('data/data file 3/data_1_smoothed.csv')\n",
    "classify_and_evaluate(df_smoothed_v0z, columns_to_focus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for Normalization:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pingk\\OneDrive - Chulalongkorn University\\Documents\\mekargit\\cpocluster\\env\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=40.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Accuracy: 0.6833333333333333\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluation for Normalization:\")\n",
    "df_normalized_v0z = pd.read_csv('data/data file 3/data_1_normalized.csv')\n",
    "classify_and_evaluate(df_normalized_v0z, columns_to_focus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for 1-Derivative Spectroscopy:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pingk\\OneDrive - Chulalongkorn University\\Documents\\mekargit\\cpocluster\\env\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=40.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Accuracy: 0.675\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluation for 1-Derivative Spectroscopy:\")\n",
    "data_1_der_combined_v0z = pd.read_csv('data/data file 3/data_1_1_der.csv')\n",
    "classify_and_evaluate(data_1_der_combined_v0z, columns_to_focus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for 2-Derivative Spectroscopy:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pingk\\OneDrive - Chulalongkorn University\\Documents\\mekargit\\cpocluster\\env\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=40.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Accuracy: 0.675\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluation for 2-Derivative Spectroscopy:\")\n",
    "data_2_der_combined_v0z = pd.read_csv('data/data file 3/data_1_2_der.csv')\n",
    "classify_and_evaluate(data_2_der_combined_v0z, columns_to_focus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for 1-SG-Derivative Spectroscopy:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pingk\\OneDrive - Chulalongkorn University\\Documents\\mekargit\\cpocluster\\env\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=40.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Accuracy: 0.6916666666666667\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluation for 1-SG-Derivative Spectroscopy:\")\n",
    "data_1_der_savgol_combined_v0z = pd.read_csv('data/data file 3/data_1_1_der_savgol.csv')\n",
    "classify_and_evaluate(data_1_der_savgol_combined_v0z, columns_to_focus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for 2-SG-Derivative Spectroscopy:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pingk\\OneDrive - Chulalongkorn University\\Documents\\mekargit\\cpocluster\\env\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=40.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Accuracy: 0.6749999999999999\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluation for 2-SG-Derivative Spectroscopy:\")\n",
    "data_1_der_savgol_combined_v0z = pd.read_csv('data/data file 3/data_1_2_der_savgol.csv')\n",
    "classify_and_evaluate(data_1_der_savgol_combined_v0z, columns_to_focus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for SNV:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pingk\\OneDrive - Chulalongkorn University\\Documents\\mekargit\\cpocluster\\env\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=40.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Accuracy: 0.7\n",
      "Evaluation for RNV:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pingk\\OneDrive - Chulalongkorn University\\Documents\\mekargit\\cpocluster\\env\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=40.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Accuracy: 0.55\n",
      "Evaluation for MSC:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pingk\\OneDrive - Chulalongkorn University\\Documents\\mekargit\\cpocluster\\env\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=40.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Accuracy: 0.725\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluation for SNV:\")\n",
    "df_snv_v0z = pd.read_csv('data/data file 3/data_1_snv.csv')\n",
    "classify_and_evaluate(df_snv_v0z, columns_to_focus)\n",
    "\n",
    "print(\"Evaluation for RNV:\")\n",
    "df_rnv_v0z = pd.read_csv('data/data file 3/data_1_rnv.csv')\n",
    "classify_and_evaluate(df_rnv_v0z, columns_to_focus)\n",
    "\n",
    "print(\"Evaluation for MSC:\")\n",
    "df_msc_v0z = pd.read_csv('data/data file 3/data_1_msc_corrected.csv')\n",
    "classify_and_evaluate(df_msc_v0z, columns_to_focus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification and Evaluation (40-fold) -with extra detailed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target variable\n",
    "target = 'country_name'\n",
    "\n",
    "# Ensure columns_to_focus are correctly identified\n",
    "numeric_cols_df_0 = df_0_selected_regions.select_dtypes(include=[np.number]).columns.tolist()\n",
    "columns_to_focus = numeric_cols_df_0  # Ensure columns are correctly selected\n",
    "\n",
    "# Classification and evaluation function using 40-fold CV with detailed metrics\n",
    "def classify_and_evaluate(df, columns):\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "    import numpy as np\n",
    "\n",
    "    # Encode target variable\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(df[target])\n",
    "\n",
    "    # Define features\n",
    "    X = df[columns]\n",
    "\n",
    "    # Initialize the classifier\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "    # Initialize Stratified K-Fold Cross-Validation\n",
    "    skf = StratifiedKFold(n_splits=40)\n",
    "\n",
    "    # Arrays to store results\n",
    "    accuracies = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "    conf_matrices = []\n",
    "\n",
    "    # Perform cross-validation\n",
    "    for train_idx, test_idx in skf.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Predict and evaluate\n",
    "        y_pred = model.predict(X_test)\n",
    "        accuracies.append(accuracy_score(y_test, y_pred))\n",
    "        precisions.append(precision_score(y_test, y_pred, average=None))\n",
    "        recalls.append(recall_score(y_test, y_pred, average=None))\n",
    "        f1_scores.append(f1_score(y_test, y_pred, average=None))\n",
    "        conf_matrices.append(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "    # Calculate mean scores\n",
    "    mean_accuracy = np.mean(accuracies)\n",
    "    mean_precision = np.mean(precisions, axis=0)\n",
    "    mean_recall = np.mean(recalls, axis=0)\n",
    "    mean_f1 = np.mean(f1_scores, axis=0)\n",
    "    mean_conf_matrix = np.mean(conf_matrices, axis=0)\n",
    "\n",
    "    # Print the results\n",
    "    print(f'Cross-Validation Accuracy: {mean_accuracy}')\n",
    "    print(f'Precision per class: {mean_precision}')\n",
    "    print(f'Recall per class: {mean_recall}')\n",
    "    print(f'F1-score per class: {mean_f1}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for No Preprocessing:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pingk\\OneDrive - Chulalongkorn University\\Documents\\mekargit\\cpocluster\\env\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=40.\n",
      "  warnings.warn(\n",
      "c:\\Users\\pingk\\OneDrive - Chulalongkorn University\\Documents\\mekargit\\cpocluster\\env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\pingk\\OneDrive - Chulalongkorn University\\Documents\\mekargit\\cpocluster\\env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\pingk\\OneDrive - Chulalongkorn University\\Documents\\mekargit\\cpocluster\\env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\pingk\\OneDrive - Chulalongkorn University\\Documents\\mekargit\\cpocluster\\env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\pingk\\OneDrive - Chulalongkorn University\\Documents\\mekargit\\cpocluster\\env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\pingk\\OneDrive - Chulalongkorn University\\Documents\\mekargit\\cpocluster\\env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\pingk\\OneDrive - Chulalongkorn University\\Documents\\mekargit\\cpocluster\\env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\pingk\\OneDrive - Chulalongkorn University\\Documents\\mekargit\\cpocluster\\env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\pingk\\OneDrive - Chulalongkorn University\\Documents\\mekargit\\cpocluster\\env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (40,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Evaluate each preprocessing method\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluation for No Preprocessing:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mclassify_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_0_selected_regions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns_to_focus\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluation for Baseline Correction:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m df_baseline_corrected_v0y \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/data file 3/data_1_bslcrct.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[24], line 54\u001b[0m, in \u001b[0;36mclassify_and_evaluate\u001b[1;34m(df, columns)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Calculate mean scores\u001b[39;00m\n\u001b[0;32m     53\u001b[0m mean_accuracy \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(accuracies)\n\u001b[1;32m---> 54\u001b[0m mean_precision \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprecisions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m mean_recall \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(recalls, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     56\u001b[0m mean_f1 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(f1_scores, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\pingk\\OneDrive - Chulalongkorn University\\Documents\\mekargit\\cpocluster\\env\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:3596\u001b[0m, in \u001b[0;36mmean\u001b[1;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[0;32m   3593\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3594\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m mean(axis\u001b[38;5;241m=\u001b[39maxis, dtype\u001b[38;5;241m=\u001b[39mdtype, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m-> 3596\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_methods\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mean\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3597\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pingk\\OneDrive - Chulalongkorn University\\Documents\\mekargit\\cpocluster\\env\\Lib\\site-packages\\numpy\\_core\\_methods.py:111\u001b[0m, in \u001b[0;36m_mean\u001b[1;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_mean\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m--> 111\u001b[0m     arr \u001b[38;5;241m=\u001b[39m \u001b[43masanyarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    113\u001b[0m     is_float16_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    115\u001b[0m     rcount \u001b[38;5;241m=\u001b[39m _count_reduce_items(arr, axis, keepdims\u001b[38;5;241m=\u001b[39mkeepdims, where\u001b[38;5;241m=\u001b[39mwhere)\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (40,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "# Evaluate each preprocessing method\n",
    "print(\"Evaluation for No Preprocessing:\")\n",
    "classify_and_evaluate(df_0_selected_regions, columns_to_focus)\n",
    "\n",
    "print(\"Evaluation for Baseline Correction:\")\n",
    "df_baseline_corrected_v0y = pd.read_csv('data/data file 3/data_1_bslcrct.csv')\n",
    "classify_and_evaluate(df_baseline_corrected_v0y, columns_to_focus)\n",
    "\n",
    "print(\"Evaluation for Smoothing:\")\n",
    "df_smoothed_v0y = pd.read_csv('data/data file 3/data_1_smoothed.csv')\n",
    "classify_and_evaluate(df_smoothed_v0y, columns_to_focus)\n",
    "\n",
    "print(\"Evaluation for Normalization:\")\n",
    "df_normalized_v0y = pd.read_csv('data/data file 3/data_1_normalized.csv')\n",
    "classify_and_evaluate(df_normalized_v0y, columns_to_focus)\n",
    "\n",
    "print(\"Evaluation for 1-Derivative Spectroscopy:\")\n",
    "df_derivative_v0y = pd.read_csv('data/data file 3/data_1_1_der.csv')\n",
    "classify_and_evaluate(df_derivative_v0y, columns_to_focus)\n",
    "\n",
    "print(\"Evaluation for 2-Derivative Spectroscopy:\")\n",
    "data_2_der_combined_v0y = pd.read_csv('data/data file 3/data_1_2_der.csv')\n",
    "classify_and_evaluate(data_2_der_combined_v0y, columns_to_focus)\n",
    "\n",
    "print(\"Evaluation for 1-SG-Derivative Spectroscopy:\")\n",
    "data_1_der_savgol_combined_v0y = pd.read_csv('data/data file 3/data_1_1_der_savgol.csv')\n",
    "classify_and_evaluate(data_1_der_savgol_combined_v0y, columns_to_focus)\n",
    "\n",
    "print(\"Evaluation for 2-SG-Derivative Spectroscopy:\")\n",
    "data_1_der_savgol_combined_v0y = pd.read_csv('data/data file 3/data_1_2_der_savgol.csv')\n",
    "classify_and_evaluate(data_1_der_savgol_combined_v0y, columns_to_focus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluation for SNV:\")\n",
    "df_snv_v0y = pd.read_csv('data/data file 3/data_1_snv.csv')\n",
    "classify_and_evaluate(df_snv_v0y, columns_to_focus)\n",
    "\n",
    "print(\"Evaluation for RNV:\")\n",
    "df_rnv_v0y = pd.read_csv('data/data file 3/data_1_rnv.csv')\n",
    "classify_and_evaluate(df_rnv_v0y, columns_to_focus)\n",
    "\n",
    "print(\"Evaluation for MSC:\")\n",
    "df_msc_v0y = pd.read_csv('data/data file 3/data_1_msc_corrected.csv')\n",
    "classify_and_evaluate(df_msc_v0y, columns_to_focus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification and Evaluation using LOGO-CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification and evaluation function using LOGO-CV with detailed metrics\n",
    "def classify_and_evaluate_logo_cv_detailed(df, columns):\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.model_selection import LeaveOneGroupOut\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "    \n",
    "    # Encode target variable\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(df[target])\n",
    "    \n",
    "    # Define features\n",
    "    X = df[columns]\n",
    "    \n",
    "    # Initialize the classifier\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    \n",
    "    # Initialize LOGO-CV\n",
    "    logo = LeaveOneGroupOut()\n",
    "    groups = df['prov_char']\n",
    "    \n",
    "    # Arrays to store results\n",
    "    accuracies = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "    all_y_test = []\n",
    "    all_y_pred = []\n",
    "    \n",
    "    # Perform LOGO-CV\n",
    "    for train_idx, test_idx in logo.split(X, y, groups=groups):\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "        \n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict and evaluate\n",
    "        y_pred = model.predict(X_test)\n",
    "        accuracies.append(accuracy_score(y_test, y_pred))\n",
    "        all_y_test.extend(y_test)\n",
    "        all_y_pred.extend(y_pred)\n",
    "        \n",
    "    # Calculate overall metrics\n",
    "    mean_accuracy = np.mean(accuracies)\n",
    "    precision = precision_score(all_y_test, all_y_pred, average=None)\n",
    "    recall = recall_score(all_y_test, all_y_pred, average=None)\n",
    "    f1 = f1_score(all_y_test, all_y_pred, average=None)\n",
    "    conf_matrix = confusion_matrix(all_y_test, all_y_pred)\n",
    "    \n",
    "    # Print the results\n",
    "    print(f'Mean Accuracy: {mean_accuracy}')\n",
    "    print(f'Precision per class: {precision}')\n",
    "    print(f'Recall per class: {recall}')\n",
    "    print(f'F1-score per class: {f1}')\n",
    "    \n",
    "    # Return confusion matrix for presentation\n",
    "    return conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate each preprocessing method using LOGO-CV with detailed metrics\n",
    "print(\"LOGO-CV Evaluation for No Preprocessing:\")\n",
    "conf_matrix_no_preprocessing_v0x = classify_and_evaluate_logo_cv_detailed(df_0_selected_regions, columns_to_focus)\n",
    "\n",
    "print(\"LOGO-CV Evaluation for Baseline Correction:\")\n",
    "df_baseline_corrected_v0x = pd.read_csv('data/data file 3/data_1_bslcrct.csv')\n",
    "conf_matrix_baseline_v0x = classify_and_evaluate_logo_cv_detailed(df_baseline_corrected_v0x, columns_to_focus)\n",
    "\n",
    "print(\"LOGO-CV Evaluation for Smoothing:\")\n",
    "df_smoothed_v0x = pd.read_csv('data/data file 3/data_1_smoothed.csv')\n",
    "conf_matrix_smoothing_v0x = classify_and_evaluate_logo_cv_detailed(df_smoothed_v0x, columns_to_focus)\n",
    "\n",
    "print(\"LOGO-CV Evaluation for Normalization:\")\n",
    "df_normalized_v0x = pd.read_csv('data/data file 3/data_1_normalized.csv')\n",
    "conf_matrix_normalization_v0x = classify_and_evaluate_logo_cv_detailed(df_normalized_v0x, columns_to_focus)\n",
    "\n",
    "print(\"LOGO-CV Evaluation for 1-Derivative Spectroscopy:\")\n",
    "df_1_derivative_v0x = pd.read_csv('data/data file 3/data_1_1_der.csv')\n",
    "conf_matrix_1_derivative_v0x = classify_and_evaluate_logo_cv_detailed(df_1_derivative_v0x, columns_to_focus)\n",
    "\n",
    "print(\"LOGO-CV Evaluation for 2-Derivative Spectroscopy:\")\n",
    "df_2_derivative_v0x = pd.read_csv('data/data file 3/data_1_2_der.csv')\n",
    "conf_matrix_2_derivative_v0x = classify_and_evaluate_logo_cv_detailed(df_2_derivative_v0x, columns_to_focus)\n",
    "\n",
    "print(\"LOGO-CV Evaluation for 1-SG-Derivative Spectroscopy:\")\n",
    "df_1_der_savgol_combined_v0y = pd.read_csv('data/data file 3/data_1_1_der_savgol.csv')\n",
    "conf_matrix_1_sg_v0x = classify_and_evaluate_logo_cv_detailed(df_1_der_savgol_combined_v0y, columns_to_focus)\n",
    "\n",
    "print(\"LOGO-CV Evaluation for 2-SG-Derivative Spectroscopy:\")\n",
    "df_2_der_savgol_combined_v0y = pd.read_csv('data/data file 3/data_1_2_der_savgol.csv')\n",
    "conf_matrix_2_sg_v0x = classify_and_evaluate_logo_cv_detailed(df_2_der_savgol_combined_v0y, columns_to_focus)\n",
    "\n",
    "print(\"LOGO-CV Evaluation for SNV:\")\n",
    "df_snv_v0x = pd.read_csv('data/data file 3/data_1_snv.csv')\n",
    "conf_matrix_snv_v0x = classify_and_evaluate_logo_cv_detailed(df_snv_v0x, columns_to_focus)\n",
    "\n",
    "print(\"LOGO-CV Evaluation for RNV:\")\n",
    "df_rnv_v0x = pd.read_csv('data/data file 3/data_1_rnv.csv')\n",
    "conf_matrix_rnv_v0x = classify_and_evaluate_logo_cv_detailed(df_rnv_v0x, columns_to_focus)\n",
    "\n",
    "print(\"LOGO-CV Evaluation for MSC:\")\n",
    "df_msc_v0x = pd.read_csv('data/data file 3/data_1_msc_corrected.csv')\n",
    "conf_matrix_msc_v0x = classify_and_evaluate_logo_cv_detailed(df_msc_v0x, columns_to_focus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display confusion matrix in a tabular format\n",
    "def display_confusion_matrix(conf_matrix, class_labels):\n",
    "    df_cm = pd.DataFrame(conf_matrix, index=class_labels, columns=class_labels)\n",
    "    print(df_cm)\n",
    "\n",
    "# Ensure LabelEncoder is defined and class labels are set\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Define the target variable and fit LabelEncoder\n",
    "target = 'country_name'\n",
    "le = LabelEncoder()\n",
    "le.fit(df_0_selected_regions[target])\n",
    "class_labels = le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display confusion matrices for each preprocessing method\n",
    "print(\"Confusion Matrix for No Preprocessing:\")\n",
    "display_confusion_matrix(conf_matrix_no_preprocessing_v0x, class_labels)\n",
    "\n",
    "print(\"Confusion Matrix for Baseline Correction:\")\n",
    "display_confusion_matrix(conf_matrix_baseline_v0x, class_labels)\n",
    "\n",
    "print(\"Confusion Matrix for Smoothing:\")\n",
    "display_confusion_matrix(conf_matrix_smoothing_v0x, class_labels)\n",
    "\n",
    "print(\"Confusion Matrix for Normalization:\")\n",
    "display_confusion_matrix(conf_matrix_normalization_v0x, class_labels)\n",
    "\n",
    "print(\"Confusion Matrix for Derivative Spectroscopy:\")\n",
    "display_confusion_matrix(conf_matrix_1_derivative_v0x, class_labels)\n",
    "\n",
    "print(\"Confusion Matrix for Derivative Spectroscopy:\")\n",
    "display_confusion_matrix(conf_matrix_2_derivative_v0x, class_labels)\n",
    "\n",
    "print(\"Confusion Matrix for Derivative Spectroscopy:\")\n",
    "display_confusion_matrix(conf_matrix_1_sg_v0x, class_labels)\n",
    "\n",
    "print(\"Confusion Matrix for Derivative Spectroscopy:\")\n",
    "display_confusion_matrix(conf_matrix_2_sg_v0x, class_labels)\n",
    "\n",
    "print(\"Confusion Matrix for SNV:\")\n",
    "display_confusion_matrix(conf_matrix_snv_v0x, class_labels)\n",
    "\n",
    "print(\"Confusion Matrix for RNV:\")\n",
    "display_confusion_matrix(conf_matrix_rnv_v0x, class_labels)\n",
    "\n",
    "print(\"Confusion Matrix for MSC:\")\n",
    "display_confusion_matrix(conf_matrix_msc_v0x, class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def snv(spectrum):\n",
    "    return (spectrum - np.mean(spectrum)) / np.std(spectrum)\n",
    "\n",
    "\n",
    "# Apply SNV to the selected regions\n",
    "df_snv_1 = df_smoothed_v0x.copy()\n",
    "for col in columns_to_focus:\n",
    "    df_snv_1[col] = snv(df_snv_1[col])\n",
    "\n",
    "# Save the SNV data\n",
    "#df_snv_1.to_csv('data/data file 4/data_1_snv.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluation for Smooth + SNV:\")\n",
    "classify_and_evaluate(df_snv_1, columns_to_focus)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
