{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and Prepare the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy import stats\n",
    "\n",
    "# Load the data\n",
    "df_0 = pd.read_csv(r\"C:\\Users\\pingk\\Downloads\\fadhli nitip\\asik_NIR_1.csv\")\n",
    "\n",
    "\n",
    "# Define the target variable and numeric columns\n",
    "target = 'tgp_name'\n",
    "numeric_cols_df_0 = df_0.select_dtypes(include=[np.number]).columns.tolist()\n",
    "string_cols_df_0 = df_0.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "\n",
    "# Drop non-numeric columns and target column from numeric columns list\n",
    "numeric_cols_df_0 = [col for col in numeric_cols_df_0 if col not in string_cols_df_0]\n",
    "\n",
    "# Specify the wavenumber regions to focus on\n",
    "focus_regions = [\n",
    "    (4641, 4681), (4867, 5510), (5657, 5826), (7057, 7097),\n",
    "    (7169, 7209), (8238, 8278)\n",
    "]\n",
    "\n",
    "# Convert column names to float where possible\n",
    "numeric_cols_df_0_float = []\n",
    "for col in numeric_cols_df_0:\n",
    "    try:\n",
    "        numeric_cols_df_0_float.append(float(col))\n",
    "    except ValueError:\n",
    "        pass  # Ignore columns that cannot be converted to float\n",
    "\n",
    "# Flatten the list of regions to get all columns in the specified regions\n",
    "columns_to_focus = []\n",
    "for start, end in focus_regions:\n",
    "    columns_to_focus.extend([col for col in numeric_cols_df_0_float if start <= col <= end])\n",
    "\n",
    "# Convert back to string for DataFrame indexing and ensure columns are present in the DataFrame\n",
    "columns_to_focus = [str(col) for col in columns_to_focus if str(col) in df_0.columns]\n",
    "\n",
    "# Display the selected columns to focus\n",
    "columns_to_focus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train RandomForest Model and Get Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "at least one array or dtype is required",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m y \u001b[38;5;241m=\u001b[39m df_0[target]\n\u001b[0;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m RandomForestClassifier(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Get feature importances\u001b[39;00m\n\u001b[0;32m      9\u001b[0m importances \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfeature_importances_\n",
      "File \u001b[1;32mc:\\Users\\pingk\\OneDrive - Chulalongkorn University\\Documents\\mekargit\\cpocluster\\env\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pingk\\OneDrive - Chulalongkorn University\\Documents\\mekargit\\cpocluster\\env\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:363\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(y):\n\u001b[0;32m    361\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse multilabel-indicator for y is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 363\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    364\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    365\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    366\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    367\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    368\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDTYPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    369\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    370\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    371\u001b[0m \u001b[38;5;66;03m# _compute_missing_values_in_feature_mask checks if X has missing values and\u001b[39;00m\n\u001b[0;32m    372\u001b[0m \u001b[38;5;66;03m# will raise an error if the underlying tree base estimator can't handle missing\u001b[39;00m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;66;03m# values. Only the criterion is required to determine if the tree supports\u001b[39;00m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;66;03m# missing values.\u001b[39;00m\n\u001b[0;32m    375\u001b[0m estimator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimator)(criterion\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion)\n",
      "File \u001b[1;32mc:\\Users\\pingk\\OneDrive - Chulalongkorn University\\Documents\\mekargit\\cpocluster\\env\\Lib\\site-packages\\sklearn\\base.py:650\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    648\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m    649\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 650\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    651\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\pingk\\OneDrive - Chulalongkorn University\\Documents\\mekargit\\cpocluster\\env\\Lib\\site-packages\\sklearn\\utils\\validation.py:1301\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1296\u001b[0m         estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[0;32m   1297\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1298\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1299\u001b[0m     )\n\u001b[1;32m-> 1301\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1302\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1303\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1304\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1306\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1307\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1308\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_writeable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1309\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1310\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1311\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1313\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1314\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1315\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1316\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1318\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[0;32m   1320\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32mc:\\Users\\pingk\\OneDrive - Chulalongkorn University\\Documents\\mekargit\\cpocluster\\env\\Lib\\site-packages\\sklearn\\utils\\validation.py:887\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    883\u001b[0m pandas_requires_conversion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28many\u001b[39m(\n\u001b[0;32m    884\u001b[0m     _pandas_dtype_needs_early_conversion(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m dtypes_orig\n\u001b[0;32m    885\u001b[0m )\n\u001b[0;32m    886\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(dtype_iter, np\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;28;01mfor\u001b[39;00m dtype_iter \u001b[38;5;129;01min\u001b[39;00m dtypes_orig):\n\u001b[1;32m--> 887\u001b[0m     dtype_orig \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult_type\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdtypes_orig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    888\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m pandas_requires_conversion \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(d \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m dtypes_orig):\n\u001b[0;32m    889\u001b[0m     \u001b[38;5;66;03m# Force object if any of the dtypes is an object\u001b[39;00m\n\u001b[0;32m    890\u001b[0m     dtype_orig \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mobject\u001b[39m\n",
      "\u001b[1;31mValueError\u001b[0m: at least one array or dtype is required"
     ]
    }
   ],
   "source": [
    "# Train a RandomForest model to get feature importances\n",
    "X = df_0[columns_to_focus]\n",
    "y = df_0[target]\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Get feature importances\n",
    "importances = model.feature_importances_\n",
    "indices = np.argsort(importances)[-15:]  # Top 15 features\n",
    "\n",
    "# Plot the distribution of the top 15 most important features\n",
    "plt.figure(figsize=(15, 20))\n",
    "for i, idx in enumerate(indices):\n",
    "    plt.subplot(5, 3, i + 1)\n",
    "    sns.histplot(df_0[columns_to_focus[idx]], bins=30, kde=True)\n",
    "    plt.title(f'Distribution of {columns_to_focus[idx]}')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove Outliers Using Z-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to remove outliers using Z-score\n",
    "def remove_outliers_zscore(df, columns, threshold=3):\n",
    "    z_scores = np.abs(stats.zscore(df[columns]))\n",
    "    return df[(z_scores < threshold).all(axis=1)]\n",
    "\n",
    "# Remove outliers from the top 15 important features\n",
    "top_features = [columns_to_focus[idx] for idx in indices]\n",
    "df_0_cleaned = remove_outliers_zscore(df_0, top_features)\n",
    "\n",
    "# Check the shape of the cleaned dataframe\n",
    "print(\"Original Data Shape:\", df_0.shape)\n",
    "print(\"Cleaned Data Shape:\", df_0_cleaned.shape)\n",
    "\n",
    "# Plot the distribution of the top 15 most important features after outlier removal\n",
    "plt.figure(figsize=(15, 20))\n",
    "for i, feature in enumerate(top_features):\n",
    "    plt.subplot(5, 3, i + 1)\n",
    "    sns.histplot(df_0_cleaned[feature], bins=30, kde=True)\n",
    "    plt.title(f'Distribution of {feature} (After Outlier Removal)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial Data Analysis and Outlier Removal Using Isolation Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a distribution plot for 'tgp_name'\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=df_0_cleaned, x=target)\n",
    "plt.title('Distribution of tgp_name')\n",
    "plt.xlabel('tgp_name')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Identify and remove outliers in each group using Isolation Forest\n",
    "iso = IsolationForest(contamination=0.1, random_state=42)\n",
    "\n",
    "# Separate samples by 'tgp_name'\n",
    "df_0_cleaned_group_1 = df_0_cleaned[df_0_cleaned[target] == 'Group 1']\n",
    "df_0_cleaned_group_2 = df_0_cleaned[df_0_cleaned[target] == 'Group 2']\n",
    "df_0_cleaned_group_3 = df_0_cleaned[df_0_cleaned[target] == 'Group 3']\n",
    "\n",
    "# Apply Isolation Forest to each group\n",
    "def remove_outliers_iso(df, numeric_cols):\n",
    "    yhat = iso.fit_predict(df[numeric_cols])\n",
    "    mask = yhat != -1\n",
    "    return df[mask]\n",
    "\n",
    "df_0_cleaned_group_1_cleaned = remove_outliers_iso(df_0_cleaned_group_1, numeric_cols_df_0)\n",
    "df_0_cleaned_group_2_cleaned = remove_outliers_iso(df_0_cleaned_group_2, numeric_cols_df_0)\n",
    "df_0_cleaned_group_3_cleaned = remove_outliers_iso(df_0_cleaned_group_3, numeric_cols_df_0)\n",
    "\n",
    "# Combine cleaned groups\n",
    "df_1_cleaned = pd.concat([df_0_cleaned_group_1_cleaned, df_0_cleaned_group_2_cleaned, df_0_cleaned_group_3_cleaned], axis=0).reset_index(drop=True)\n",
    "\n",
    "# Plot the distribution of tgp_name after outlier removal\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=df_1_cleaned, x=target)\n",
    "plt.title('Distribution of tgp_name after Outlier Removal')\n",
    "plt.xlabel('tgp_name')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# Verify the new distribution\n",
    "print(\"New distribution of tgp_name:\")\n",
    "print(df_1_cleaned[target].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode the Target Variable and Balance the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = df_1_cleaned.copy()\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encoding the 'tgp_name' column\n",
    "le = LabelEncoder()\n",
    "df_2['tgp_name_encoded'] = le.fit_transform(df_2['tgp_name'])\n",
    "\n",
    "# Define the target variable and numeric columns\n",
    "numeric_cols_df_2 = df_2.select_dtypes(include=[np.number]).columns.tolist()\n",
    "string_cols_df_2 = df_2.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "\n",
    "# Drop non-numeric columns and target column from numeric columns list\n",
    "numeric_cols_df_2 = [col for col in numeric_cols_df_2 if col not in string_cols_df_2]\n",
    "\n",
    "# Train a RandomForest model to get feature importances\n",
    "X_df_2 = df_2[numeric_cols_df_2].drop(columns=['tgp_name_encoded'])\n",
    "y_df_2 = df_2['tgp_name_encoded'].values\n",
    "\n",
    "# Initial model training to find consistent samples\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_df_2, y_df_2)\n",
    "\n",
    "# Get predictions\n",
    "df_2['predictions'] = rf.predict(X_df_2)\n",
    "\n",
    "# Keep samples that are correctly classified\n",
    "consistent_samples = df_2[df_2['tgp_name_encoded'] == df_2['predictions']]\n",
    "\n",
    "# Separate features and target again with consistent samples\n",
    "X_df_2_consistent = consistent_samples.drop(columns=['tgp_name', 'tgp_name_encoded', 'predictions'])\n",
    "y_df_2_consistent = consistent_samples['tgp_name_encoded'].values\n",
    "\n",
    "# Ensure X_df_2_consistent contains only numeric columns\n",
    "X_df_2_consistent = X_df_2_consistent.select_dtypes(include=[np.number])\n",
    "\n",
    "# Determine the size of the smallest group\n",
    "min_size = consistent_samples['tgp_name'].value_counts().min()\n",
    "\n",
    "# Map for original string labels to numeric encoded labels\n",
    "label_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "\n",
    "# Apply undersampling to achieve a balanced dataset\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Create the sampling strategy dictionary\n",
    "sampling_strategy = {label_mapping[key]: min_size for key in label_mapping}\n",
    "\n",
    "rus = RandomUnderSampler(sampling_strategy=sampling_strategy, random_state=42)\n",
    "X_df_2_balanced, y_df_2_balanced = rus.fit_resample(X_df_2_consistent, y_df_2_consistent)\n",
    "\n",
    "# Display the new distribution of 'tgp_name'\n",
    "balanced_df = consistent_samples.iloc[rus.sample_indices_]\n",
    "tgp_name_distribution = balanced_df['tgp_name'].value_counts()\n",
    "print(tgp_name_distribution)\n",
    "\n",
    "# Check the distribution of 'tgp_name' after undersampling\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=balanced_df, x='tgp_name')\n",
    "plt.title('Distribution of tgp_name after Undersampling')\n",
    "plt.xlabel('tgp_name')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-Validation and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "\n",
    "# Initialize and encode the 'prov_char' column\n",
    "le_prov = LabelEncoder()\n",
    "prov_char_encoded = le_prov.fit_transform(balanced_df['prov_char'])\n",
    "\n",
    "# Define features and target for the balanced dataset\n",
    "X_balanced = balanced_df.drop(columns=['tgp_name', 'tgp_name_encoded'])\n",
    "y_balanced = balanced_df['tgp_name_encoded'].values\n",
    "\n",
    "# Ensure X_balanced contains only numeric columns\n",
    "X_balanced = X_balanced.select_dtypes(include=[np.number])\n",
    "\n",
    "# Initialize LOGO-CV\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "# Arrays to store results\n",
    "accuracies = []\n",
    "left_out_groups = []\n",
    "all_y_test = []\n",
    "all_y_pred = []\n",
    "\n",
    "# Perform LOGO-CV\n",
    "for train_idx, test_idx in logo.split(X_balanced, y_balanced, groups=prov_char_encoded):\n",
    "    X_train, X_test = X_balanced.iloc[train_idx], X_balanced.iloc[test_idx]\n",
    "    y_train, y_test = y_balanced[train_idx], y_balanced[test_idx]\n",
    "    \n",
    "    # Train the model\n",
    "    rf.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict and evaluate\n",
    "    y_pred = rf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Store results\n",
    "    accuracies.append(accuracy)\n",
    "    left_out_groups.append(balanced_df['prov_char'].iloc[test_idx].unique())\n",
    "    all_y_test.extend(y_test)\n",
    "    all_y_pred.extend(y_pred)\n",
    "\n",
    "# Calculate mean accuracy\n",
    "mean_accuracy = np.mean(accuracies)\n",
    "print(f'Mean Accuracy: {mean_accuracy}')\n",
    "\n",
    "# Display left out groups and their accuracies\n",
    "for group, acc in zip(left_out_groups, accuracies):\n",
    "    print(f'Left out group: {group}, Accuracy: {acc}')\n",
    "\n",
    "# Generate confusion matrix\n",
    "conf_matrix = confusion_matrix(all_y_test, all_y_pred)\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Cleanup and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Rename the column 'wavenumber' to 'sample_code'\n",
    "data_1 = balanced_df.rename(columns={'wavenumber': 'sample_code'})\n",
    "\n",
    "data_1 = balanced_df.drop(columns=['tgp_name_encoded', 'predictions'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1.drop(columns=['country_char'], inplace=True)\n",
    "\n",
    "data_1['sample_code'] = data_1['sample_code'].str[:19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_1.to_csv(r\"C:\\Users\\pingk\\Downloads\\fadhli nitip\\asikrt4_3g2_clnd_blncd.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
