{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df_0 = pd.read_csv(r\"C:\\Users\\pingk\\Downloads\\fadhli nitip\\asik_NIR_DIST_3b.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prov_char</th>\n",
       "      <th>country_char</th>\n",
       "      <th>wavenumber</th>\n",
       "      <th>11544</th>\n",
       "      <th>11540</th>\n",
       "      <th>11536</th>\n",
       "      <th>11532</th>\n",
       "      <th>11528</th>\n",
       "      <th>11524</th>\n",
       "      <th>11520</th>\n",
       "      <th>...</th>\n",
       "      <th>3964</th>\n",
       "      <th>3960</th>\n",
       "      <th>3956</th>\n",
       "      <th>3952</th>\n",
       "      <th>tgp_name</th>\n",
       "      <th>dgp_name</th>\n",
       "      <th>fgp_name</th>\n",
       "      <th>country_name</th>\n",
       "      <th>thnoth_name</th>\n",
       "      <th>thnoth_name_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KAX</td>\n",
       "      <td>ID</td>\n",
       "      <td>ID-KAX-088-2401-001</td>\n",
       "      <td>0.290252</td>\n",
       "      <td>0.290122</td>\n",
       "      <td>0.290044</td>\n",
       "      <td>0.290029</td>\n",
       "      <td>0.290042</td>\n",
       "      <td>0.289978</td>\n",
       "      <td>0.289851</td>\n",
       "      <td>...</td>\n",
       "      <td>3.558086</td>\n",
       "      <td>3.542757</td>\n",
       "      <td>3.530849</td>\n",
       "      <td>3.501222</td>\n",
       "      <td>Group 3</td>\n",
       "      <td>Group 2</td>\n",
       "      <td>Group 4</td>\n",
       "      <td>Indonesia</td>\n",
       "      <td>Non-Thai</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KBX</td>\n",
       "      <td>ID</td>\n",
       "      <td>ID-KBX-068-2306-003</td>\n",
       "      <td>0.522845</td>\n",
       "      <td>0.522798</td>\n",
       "      <td>0.522826</td>\n",
       "      <td>0.522669</td>\n",
       "      <td>0.522414</td>\n",
       "      <td>0.522217</td>\n",
       "      <td>0.522070</td>\n",
       "      <td>...</td>\n",
       "      <td>3.680422</td>\n",
       "      <td>3.655416</td>\n",
       "      <td>3.629388</td>\n",
       "      <td>3.606223</td>\n",
       "      <td>Group 3</td>\n",
       "      <td>Group 2</td>\n",
       "      <td>Group 4</td>\n",
       "      <td>Indonesia</td>\n",
       "      <td>Non-Thai</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BBX</td>\n",
       "      <td>ID</td>\n",
       "      <td>ID-BBX-095-2401-004</td>\n",
       "      <td>-0.144108</td>\n",
       "      <td>-0.144008</td>\n",
       "      <td>-0.143893</td>\n",
       "      <td>-0.143777</td>\n",
       "      <td>-0.143686</td>\n",
       "      <td>-0.143667</td>\n",
       "      <td>-0.143678</td>\n",
       "      <td>...</td>\n",
       "      <td>3.650478</td>\n",
       "      <td>3.623180</td>\n",
       "      <td>3.614590</td>\n",
       "      <td>3.581875</td>\n",
       "      <td>Group 3</td>\n",
       "      <td>Group 2</td>\n",
       "      <td>Group 3</td>\n",
       "      <td>Indonesia</td>\n",
       "      <td>Non-Thai</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SUX</td>\n",
       "      <td>ID</td>\n",
       "      <td>ID-SUX-074-2311-001</td>\n",
       "      <td>0.333905</td>\n",
       "      <td>0.333777</td>\n",
       "      <td>0.333785</td>\n",
       "      <td>0.333700</td>\n",
       "      <td>0.333522</td>\n",
       "      <td>0.333453</td>\n",
       "      <td>0.333446</td>\n",
       "      <td>...</td>\n",
       "      <td>3.601331</td>\n",
       "      <td>3.566453</td>\n",
       "      <td>3.539593</td>\n",
       "      <td>3.512285</td>\n",
       "      <td>Group 3</td>\n",
       "      <td>Group 2</td>\n",
       "      <td>Group 3</td>\n",
       "      <td>Indonesia</td>\n",
       "      <td>Non-Thai</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SUX</td>\n",
       "      <td>ID</td>\n",
       "      <td>ID-SUX-073-2311-001</td>\n",
       "      <td>1.522433</td>\n",
       "      <td>1.521460</td>\n",
       "      <td>1.521133</td>\n",
       "      <td>1.521072</td>\n",
       "      <td>1.521494</td>\n",
       "      <td>1.522300</td>\n",
       "      <td>1.521840</td>\n",
       "      <td>...</td>\n",
       "      <td>4.103365</td>\n",
       "      <td>4.125110</td>\n",
       "      <td>4.113971</td>\n",
       "      <td>4.055192</td>\n",
       "      <td>Group 3</td>\n",
       "      <td>Group 2</td>\n",
       "      <td>Group 3</td>\n",
       "      <td>Indonesia</td>\n",
       "      <td>Non-Thai</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1908 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  prov_char country_char           wavenumber     11544     11540     11536  \\\n",
       "0       KAX           ID  ID-KAX-088-2401-001  0.290252  0.290122  0.290044   \n",
       "1       KBX           ID  ID-KBX-068-2306-003  0.522845  0.522798  0.522826   \n",
       "2       BBX           ID  ID-BBX-095-2401-004 -0.144108 -0.144008 -0.143893   \n",
       "3       SUX           ID  ID-SUX-074-2311-001  0.333905  0.333777  0.333785   \n",
       "4       SUX           ID  ID-SUX-073-2311-001  1.522433  1.521460  1.521133   \n",
       "\n",
       "      11532     11528     11524     11520  ...      3964      3960      3956  \\\n",
       "0  0.290029  0.290042  0.289978  0.289851  ...  3.558086  3.542757  3.530849   \n",
       "1  0.522669  0.522414  0.522217  0.522070  ...  3.680422  3.655416  3.629388   \n",
       "2 -0.143777 -0.143686 -0.143667 -0.143678  ...  3.650478  3.623180  3.614590   \n",
       "3  0.333700  0.333522  0.333453  0.333446  ...  3.601331  3.566453  3.539593   \n",
       "4  1.521072  1.521494  1.522300  1.521840  ...  4.103365  4.125110  4.113971   \n",
       "\n",
       "       3952  tgp_name  dgp_name  fgp_name  country_name  thnoth_name  \\\n",
       "0  3.501222   Group 3   Group 2   Group 4     Indonesia     Non-Thai   \n",
       "1  3.606223   Group 3   Group 2   Group 4     Indonesia     Non-Thai   \n",
       "2  3.581875   Group 3   Group 2   Group 3     Indonesia     Non-Thai   \n",
       "3  3.512285   Group 3   Group 2   Group 3     Indonesia     Non-Thai   \n",
       "4  4.055192   Group 3   Group 2   Group 3     Indonesia     Non-Thai   \n",
       "\n",
       "   thnoth_name_encoded  \n",
       "0                    0  \n",
       "1                    0  \n",
       "2                    0  \n",
       "3                    0  \n",
       "4                    0  \n",
       "\n",
       "[5 rows x 1908 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_0.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting Regions of Interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the regions of interest\n",
    "regions_of_interest = [\n",
    "    (4641, 4681), (4867, 5510), (5657, 5826), (7057, 7097),\n",
    "    (7169, 7209), (8238, 8278)\n",
    "]\n",
    "\n",
    "# Extract columns corresponding to the regions of interest\n",
    "columns_to_focus = []\n",
    "for start, end in regions_of_interest:\n",
    "    columns_to_focus.extend([col for col in df_0.columns[4:-7] if start <= float(col) <= end])\n",
    "\n",
    "# Create a new DataFrame with the selected regions\n",
    "df_0_selected_regions = df_0[columns_to_focus]\n",
    "\n",
    "# Combine the selected regions with the target column and other relevant columns\n",
    "df_0_selected_regions = pd.concat([df_0[['thnoth_name', 'prov_char']], df_0_selected_regions], axis=1)\n",
    "\n",
    "# Save the DataFrame for further processing\n",
    "df_0_selected_regions.to_csv('data/data file 2b/data_1.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>thnoth_name</th>\n",
       "      <th>prov_char</th>\n",
       "      <th>4680</th>\n",
       "      <th>4676</th>\n",
       "      <th>4672</th>\n",
       "      <th>4668</th>\n",
       "      <th>4664</th>\n",
       "      <th>4660</th>\n",
       "      <th>4656</th>\n",
       "      <th>4652</th>\n",
       "      <th>...</th>\n",
       "      <th>8276</th>\n",
       "      <th>8272</th>\n",
       "      <th>8268</th>\n",
       "      <th>8264</th>\n",
       "      <th>8260</th>\n",
       "      <th>8256</th>\n",
       "      <th>8252</th>\n",
       "      <th>8248</th>\n",
       "      <th>8244</th>\n",
       "      <th>8240</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Non-Thai</td>\n",
       "      <td>KAX</td>\n",
       "      <td>1.145221</td>\n",
       "      <td>1.161998</td>\n",
       "      <td>1.183733</td>\n",
       "      <td>1.206279</td>\n",
       "      <td>1.221476</td>\n",
       "      <td>1.219369</td>\n",
       "      <td>1.195433</td>\n",
       "      <td>1.155057</td>\n",
       "      <td>...</td>\n",
       "      <td>0.693094</td>\n",
       "      <td>0.696532</td>\n",
       "      <td>0.699208</td>\n",
       "      <td>0.701041</td>\n",
       "      <td>0.702028</td>\n",
       "      <td>0.702149</td>\n",
       "      <td>0.701310</td>\n",
       "      <td>0.699461</td>\n",
       "      <td>0.696617</td>\n",
       "      <td>0.692720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Non-Thai</td>\n",
       "      <td>KBX</td>\n",
       "      <td>1.100158</td>\n",
       "      <td>1.117663</td>\n",
       "      <td>1.140348</td>\n",
       "      <td>1.163641</td>\n",
       "      <td>1.178847</td>\n",
       "      <td>1.175626</td>\n",
       "      <td>1.149462</td>\n",
       "      <td>1.106238</td>\n",
       "      <td>...</td>\n",
       "      <td>0.827193</td>\n",
       "      <td>0.830521</td>\n",
       "      <td>0.833110</td>\n",
       "      <td>0.834886</td>\n",
       "      <td>0.835800</td>\n",
       "      <td>0.835858</td>\n",
       "      <td>0.835008</td>\n",
       "      <td>0.833129</td>\n",
       "      <td>0.830211</td>\n",
       "      <td>0.826278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Non-Thai</td>\n",
       "      <td>BBX</td>\n",
       "      <td>0.873460</td>\n",
       "      <td>0.891096</td>\n",
       "      <td>0.914072</td>\n",
       "      <td>0.937998</td>\n",
       "      <td>0.954190</td>\n",
       "      <td>0.951890</td>\n",
       "      <td>0.926066</td>\n",
       "      <td>0.882441</td>\n",
       "      <td>...</td>\n",
       "      <td>0.370219</td>\n",
       "      <td>0.373736</td>\n",
       "      <td>0.376443</td>\n",
       "      <td>0.378311</td>\n",
       "      <td>0.379332</td>\n",
       "      <td>0.379466</td>\n",
       "      <td>0.378665</td>\n",
       "      <td>0.376875</td>\n",
       "      <td>0.374042</td>\n",
       "      <td>0.370139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Non-Thai</td>\n",
       "      <td>SUX</td>\n",
       "      <td>1.014767</td>\n",
       "      <td>1.031323</td>\n",
       "      <td>1.052854</td>\n",
       "      <td>1.075045</td>\n",
       "      <td>1.089551</td>\n",
       "      <td>1.086283</td>\n",
       "      <td>1.060706</td>\n",
       "      <td>1.018413</td>\n",
       "      <td>...</td>\n",
       "      <td>0.694977</td>\n",
       "      <td>0.698438</td>\n",
       "      <td>0.701122</td>\n",
       "      <td>0.702996</td>\n",
       "      <td>0.703988</td>\n",
       "      <td>0.704090</td>\n",
       "      <td>0.703278</td>\n",
       "      <td>0.701477</td>\n",
       "      <td>0.698633</td>\n",
       "      <td>0.694729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Non-Thai</td>\n",
       "      <td>SUX</td>\n",
       "      <td>1.520511</td>\n",
       "      <td>1.536409</td>\n",
       "      <td>1.557498</td>\n",
       "      <td>1.579159</td>\n",
       "      <td>1.592749</td>\n",
       "      <td>1.588327</td>\n",
       "      <td>1.561828</td>\n",
       "      <td>1.519022</td>\n",
       "      <td>...</td>\n",
       "      <td>1.606612</td>\n",
       "      <td>1.609559</td>\n",
       "      <td>1.611674</td>\n",
       "      <td>1.613343</td>\n",
       "      <td>1.614402</td>\n",
       "      <td>1.614270</td>\n",
       "      <td>1.612957</td>\n",
       "      <td>1.610735</td>\n",
       "      <td>1.607516</td>\n",
       "      <td>1.603303</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 245 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  thnoth_name prov_char      4680      4676      4672      4668      4664  \\\n",
       "0    Non-Thai       KAX  1.145221  1.161998  1.183733  1.206279  1.221476   \n",
       "1    Non-Thai       KBX  1.100158  1.117663  1.140348  1.163641  1.178847   \n",
       "2    Non-Thai       BBX  0.873460  0.891096  0.914072  0.937998  0.954190   \n",
       "3    Non-Thai       SUX  1.014767  1.031323  1.052854  1.075045  1.089551   \n",
       "4    Non-Thai       SUX  1.520511  1.536409  1.557498  1.579159  1.592749   \n",
       "\n",
       "       4660      4656      4652  ...      8276      8272      8268      8264  \\\n",
       "0  1.219369  1.195433  1.155057  ...  0.693094  0.696532  0.699208  0.701041   \n",
       "1  1.175626  1.149462  1.106238  ...  0.827193  0.830521  0.833110  0.834886   \n",
       "2  0.951890  0.926066  0.882441  ...  0.370219  0.373736  0.376443  0.378311   \n",
       "3  1.086283  1.060706  1.018413  ...  0.694977  0.698438  0.701122  0.702996   \n",
       "4  1.588327  1.561828  1.519022  ...  1.606612  1.609559  1.611674  1.613343   \n",
       "\n",
       "       8260      8256      8252      8248      8244      8240  \n",
       "0  0.702028  0.702149  0.701310  0.699461  0.696617  0.692720  \n",
       "1  0.835800  0.835858  0.835008  0.833129  0.830211  0.826278  \n",
       "2  0.379332  0.379466  0.378665  0.376875  0.374042  0.370139  \n",
       "3  0.703988  0.704090  0.703278  0.701477  0.698633  0.694729  \n",
       "4  1.614402  1.614270  1.612957  1.610735  1.607516  1.603303  \n",
       "\n",
       "[5 rows x 245 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_0_selected_regions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FUNCTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import savgol_filter\n",
    "\n",
    "# Function for baseline correction with dynamic window length\n",
    "def baseline_correction(spectrum, default_window_length=15, polyorder=3):\n",
    "    spectrum_length = len(spectrum)\n",
    "    if spectrum_length < default_window_length:\n",
    "        window_length = spectrum_length // 2 * 2 + 1  # Make window length odd and less than the size of the spectrum\n",
    "    else:\n",
    "        window_length = default_window_length\n",
    "    baseline = savgol_filter(spectrum, window_length, polyorder, mode='nearest')\n",
    "    corrected_spectrum = spectrum - baseline\n",
    "    return corrected_spectrum\n",
    "\n",
    "# Apply baseline correction\n",
    "df_baseline_corrected_v0 = df_0_selected_regions.copy()\n",
    "for col in columns_to_focus:\n",
    "    df_baseline_corrected_v0[col] = baseline_correction(df_baseline_corrected_v0[col])\n",
    "\n",
    "# Save the baseline corrected data\n",
    "df_baseline_corrected_v0.to_csv('data/data file 2b/data_1_bslcrct.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SavGol Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for Savitzky-Golay smoothing\n",
    "def savitzky_golay_smoothing(spectrum, default_window_length=11, polyorder=2):\n",
    "    window_length = min(default_window_length, len(spectrum) // 2 * 2 + 1)  # Make window length odd and less than or equal to the size of the spectrum\n",
    "    if window_length < 3:  # Ensure window length is at least 3\n",
    "        window_length = 3\n",
    "    return savgol_filter(spectrum, window_length, polyorder, mode='nearest')  # Set mode to 'nearest'\n",
    "\n",
    "# Apply smoothing\n",
    "df_smoothed_v0 = df_baseline_corrected_v0.copy()\n",
    "for col in columns_to_focus:\n",
    "    df_smoothed_v0[col] = savitzky_golay_smoothing(df_smoothed_v0[col])\n",
    "\n",
    "# Save the smoothed data\n",
    "df_smoothed_v0.to_csv('data/data file 2b/data_1_smoothed.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for normalization (Min-Max scaling)\n",
    "def min_max_normalization(spectrum):\n",
    "    return (spectrum - np.min(spectrum)) / (np.max(spectrum) - np.min(spectrum))\n",
    "\n",
    "# Apply normalization\n",
    "df_normalized_v0 = df_smoothed_v0.copy()\n",
    "for col in columns_to_focus:\n",
    "    df_normalized_v0[col] = min_max_normalization(df_normalized_v0[col])\n",
    "\n",
    "# Save the normalized data\n",
    "df_normalized_v0.to_csv('data/data file 2b/data_1_normalized.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derivatization (np.gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the first derivative using np.gradient\n",
    "data_spectrum = df_normalized_v0.iloc[:, 2:].values\n",
    "first_derivative_np = np.gradient(data_spectrum, axis=1)\n",
    "\n",
    "# Calculate the second derivative using np.gradient\n",
    "second_derivative_np = np.gradient(first_derivative_np, axis=1)\n",
    "\n",
    "# Convert the results back to DataFrame\n",
    "data_1_der_np = pd.DataFrame(first_derivative_np, columns=df_normalized_v0.columns[2:])\n",
    "data_2_der_np = pd.DataFrame(second_derivative_np, columns=df_normalized_v0.columns[2:])\n",
    "\n",
    "# Combine the first two columns from the original dataset with the np.gradient derivatives\n",
    "\n",
    "# Extract the first two columns\n",
    "first_two_columns = df_normalized_v0.iloc[:, :2]\n",
    "\n",
    "# Combine the first two columns with the derivatives\n",
    "data_1_der_combined = pd.concat([first_two_columns, data_1_der_np], axis=1)\n",
    "data_2_der_combined = pd.concat([first_two_columns, data_2_der_np], axis=1)\n",
    "\n",
    "# Export the combined data to CSV\n",
    "data_1_der_combined.to_csv('data/data file 2b/data_1_1_der.csv', index=False)\n",
    "data_2_der_combined.to_csv('data/data file 2b/data_1_2_der.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derivatization (SavGol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the spectrum data\n",
    "data_spectrum = df_normalized_v0.iloc[:, 2:].values\n",
    "\n",
    "# Apply Savitzky-Golay filter for the first derivative\n",
    "first_derivative_savgol = savgol_filter(data_spectrum, window_length=5, polyorder=2, deriv=1, axis=1)\n",
    "\n",
    "# Apply Savitzky-Golay filter for the second derivative\n",
    "second_derivative_savgol = savgol_filter(data_spectrum, window_length=5, polyorder=2, deriv=2, axis=1)\n",
    "\n",
    "# Convert the results back to DataFrame\n",
    "data_1_der_savgol = pd.DataFrame(first_derivative_savgol, columns=df_normalized_v0.columns[2:])\n",
    "data_2_der_savgol = pd.DataFrame(second_derivative_savgol, columns=df_normalized_v0.columns[2:])\n",
    "\n",
    "# Extract the first two columns\n",
    "first_two_columns = df_normalized_v0.iloc[:, :2]\n",
    "\n",
    "# Combine the first two columns with the Savitzky-Golay derivatives\n",
    "data_1_der_savgol_combined = pd.concat([first_two_columns, data_1_der_savgol], axis=1)\n",
    "data_2_der_savgol_combined = pd.concat([first_two_columns, data_2_der_savgol], axis=1)\n",
    "\n",
    "# Export the combined data to CSV\n",
    "data_1_der_savgol_combined.to_csv('data/data file 2b/data_1_1_der_savgol.csv', index=False)\n",
    "data_2_der_savgol_combined.to_csv('data/data file 2b/data_1_2_der_savgol.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SNV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def snv(spectrum):\n",
    "    return (spectrum - np.mean(spectrum)) / np.std(spectrum)\n",
    "\n",
    "# Apply SNV to the selected regions\n",
    "df_snv = df_0_selected_regions.copy()\n",
    "for col in columns_to_focus:\n",
    "    df_snv[col] = snv(df_snv[col])\n",
    "\n",
    "# Save the SNV data\n",
    "df_snv.to_csv('data/data file 2b/data_1_snv.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Normal Variate (RNV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnv(spectrum):\n",
    "    random_noise = np.random.normal(0, np.std(spectrum), spectrum.shape)\n",
    "    return spectrum + random_noise\n",
    "\n",
    "# Apply RNV to the selected regions\n",
    "df_rnv = df_0_selected_regions.copy()\n",
    "for col in columns_to_focus:\n",
    "    df_rnv[col] = rnv(df_rnv[col])\n",
    "\n",
    "# Save the RNV data\n",
    "df_rnv.to_csv('data/data file 2b/data_1_rnv.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiplicative Scatter Correction (MSC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the spectral df_0_selected_regions columns (excluding 'thnoth_name', 'prov_char', and the last two columns)\n",
    "spectral_df_0_selected_regions_columns = df_0_selected_regions.columns[2:]\n",
    "spectral_df_0_selected_regions = df_0_selected_regions[spectral_df_0_selected_regions_columns]\n",
    "\n",
    "# Calculate the mean spectrum across all samples\n",
    "mean_spectrum = spectral_df_0_selected_regions.mean(axis=0)\n",
    "\n",
    "# Perform Mean Centering (MSC)\n",
    "msc_spectral_df_0_selected_regions = spectral_df_0_selected_regions - mean_spectrum\n",
    "\n",
    "# Add back the non-spectral columns to the df_0_selected_regionsframe\n",
    "msc_df_0_selected_regions = df_0_selected_regions[['thnoth_name', 'prov_char']].copy()\n",
    "msc_df_0_selected_regions = pd.concat([msc_df_0_selected_regions, msc_spectral_df_0_selected_regions], axis=1)\n",
    "\n",
    "# Save the MSC preprocessed df_0_selected_regions to a new CSV file\n",
    "msc_df_0_selected_regions.to_csv('data/data file 2b/data_1_msc.csv', index=False)\n",
    "\n",
    "# Display the first few rows of the MSC preprocessed df_0_selected_regions\n",
    "#print(msc_df_0_selected_regions.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification and Evaluation (40-fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target variable\n",
    "target = 'thnoth_name'\n",
    "\n",
    "# Ensure columns_to_focus are correctly identified\n",
    "numeric_cols_df_0 = df_0_selected_regions.select_dtypes(include=[np.number]).columns.tolist()\n",
    "columns_to_focus = numeric_cols_df_0  # Ensure columns are correctly selected\n",
    "\n",
    "# Classification and evaluation function using 40-fold CV\n",
    "def classify_and_evaluate(df, columns):\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "    # Encode target variable\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(df[target])\n",
    "\n",
    "    # Define features\n",
    "    X = df[columns]\n",
    "\n",
    "    # Initialize the classifier\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "    # Perform cross-validation\n",
    "    scores = cross_val_score(model, X, y, cv=40)\n",
    "\n",
    "    # Print the results\n",
    "    print(f'Cross-Validation Accuracy: {np.mean(scores)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for No Preprocessing:\n",
      "Cross-Validation Accuracy: 0.6871829710144928\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluate each preprocessing method\n",
    "print(\"Evaluation for No Preprocessing:\")\n",
    "classify_and_evaluate(df_0_selected_regions, columns_to_focus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for Baseline Correction:\n",
      "Cross-Validation Accuracy: 0.5912590579710144\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluation for Baseline Correction:\")\n",
    "df_baseline_corrected_v01 = pd.read_csv('data/data file 2b/data_1_bslcrct.csv')\n",
    "classify_and_evaluate(df_baseline_corrected_v01, columns_to_focus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for Smoothing:\n",
      "Cross-Validation Accuracy: 0.5933876811594203\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluation for Smoothing:\")\n",
    "df_smoothed_v01 = pd.read_csv('data/data file 2b/data_1_smoothed.csv')\n",
    "classify_and_evaluate(df_smoothed_v01, columns_to_focus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for Normalization:\n",
      "Cross-Validation Accuracy: 0.5933876811594203\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluation for Normalization:\")\n",
    "df_normalized_v01 = pd.read_csv('data/data file 2b/data_1_normalized.csv')\n",
    "classify_and_evaluate(df_normalized_v01, columns_to_focus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for 1-Derivative Spectroscopy:\n",
      "Cross-Validation Accuracy: 0.8411684782608695\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluation for 1-Derivative Spectroscopy:\")\n",
    "data_1_der_combined_v01 = pd.read_csv('data/data file 2b/data_1_1_der.csv')\n",
    "classify_and_evaluate(data_1_der_combined_v01, columns_to_focus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for 2-Derivative Spectroscopy:\n",
      "Cross-Validation Accuracy: 0.8201539855072465\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluation for 2-Derivative Spectroscopy:\")\n",
    "data_2_der_combined_v01 = pd.read_csv('data/data file 2b/data_1_2_der.csv')\n",
    "classify_and_evaluate(data_2_der_combined_v01, columns_to_focus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for 1-SG-Derivative Spectroscopy:\n",
      "Cross-Validation Accuracy: 0.8316576086956522\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluation for 1-SG-Derivative Spectroscopy:\")\n",
    "data_1_der_savgol_combined_v01 = pd.read_csv('data/data file 2b/data_1_1_der_savgol.csv')\n",
    "classify_and_evaluate(data_1_der_savgol_combined_v01, columns_to_focus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for 2-SG-Derivative Spectroscopy:\n",
      "Cross-Validation Accuracy: 0.8287137681159422\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluation for 2-SG-Derivative Spectroscopy:\")\n",
    "data_2_der_savgol_combined_v01 = pd.read_csv('data/data file 2b/data_1_2_der_savgol.csv')\n",
    "classify_and_evaluate(data_2_der_savgol_combined_v01, columns_to_focus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for SNV:\n",
      "Cross-Validation Accuracy: 0.6871829710144928\n",
      "Evaluation for RNV:\n",
      "Cross-Validation Accuracy: 0.5071557971014492\n",
      "Evaluation for MSC:\n",
      "Cross-Validation Accuracy: 0.6871829710144928\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluation for SNV:\")\n",
    "df_snv_v01 = pd.read_csv('data/data file 2b/data_1_snv.csv')\n",
    "classify_and_evaluate(df_snv_v01, columns_to_focus)\n",
    "\n",
    "print(\"Evaluation for RNV:\")\n",
    "df_rnv_v01 = pd.read_csv('data/data file 2b/data_1_rnv.csv')\n",
    "classify_and_evaluate(df_rnv_v01, columns_to_focus)\n",
    "\n",
    "print(\"Evaluation for MSC:\")\n",
    "df_msc_v01 = pd.read_csv('data/data file 2b/data_1_msc.csv')\n",
    "classify_and_evaluate(df_msc_v01, columns_to_focus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification and Evaluation (40-fold) -with extra detailed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target variable\n",
    "target = 'thnoth_name'\n",
    "\n",
    "# Ensure columns_to_focus are correctly identified\n",
    "numeric_cols_df_0 = df_0_selected_regions.select_dtypes(include=[np.number]).columns.tolist()\n",
    "columns_to_focus = numeric_cols_df_0  # Ensure columns are correctly selected\n",
    "\n",
    "# Classification and evaluation function using 40-fold CV with detailed metrics\n",
    "def classify_and_evaluate(df, columns):\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "    import numpy as np\n",
    "\n",
    "    # Encode target variable\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(df[target])\n",
    "\n",
    "    # Define features\n",
    "    X = df[columns]\n",
    "\n",
    "    # Initialize the classifier\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "    # Initialize Stratified K-Fold Cross-Validation\n",
    "    skf = StratifiedKFold(n_splits=40)\n",
    "\n",
    "    # Arrays to store results\n",
    "    accuracies = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "    conf_matrices = []\n",
    "\n",
    "    # Perform cross-validation\n",
    "    for train_idx, test_idx in skf.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Predict and evaluate\n",
    "        y_pred = model.predict(X_test)\n",
    "        accuracies.append(accuracy_score(y_test, y_pred))\n",
    "        precisions.append(precision_score(y_test, y_pred, average=None))\n",
    "        recalls.append(recall_score(y_test, y_pred, average=None))\n",
    "        f1_scores.append(f1_score(y_test, y_pred, average=None))\n",
    "        conf_matrices.append(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "    # Calculate mean scores\n",
    "    mean_accuracy = np.mean(accuracies)\n",
    "    mean_precision = np.mean(precisions, axis=0)\n",
    "    mean_recall = np.mean(recalls, axis=0)\n",
    "    mean_f1 = np.mean(f1_scores, axis=0)\n",
    "    mean_conf_matrix = np.mean(conf_matrices, axis=0)\n",
    "\n",
    "    # Print the results\n",
    "    print(f'Cross-Validation Accuracy: {mean_accuracy}')\n",
    "    print(f'Precision per class: {mean_precision}')\n",
    "    print(f'Recall per class: {mean_recall}')\n",
    "    print(f'F1-score per class: {mean_f1}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for No Preprocessing:\n",
      "Cross-Validation Accuracy: 0.6871829710144928\n",
      "Precision per class: [0.7089437  0.67677392]\n",
      "Recall per class: [0.64772727 0.72613636]\n",
      "F1-score per class: [0.67206709 0.69703752]\n",
      "Evaluation for Baseline Correction:\n",
      "Cross-Validation Accuracy: 0.5912590579710144\n",
      "Precision per class: [0.59410964 0.59282087]\n",
      "Recall per class: [0.54109848 0.6405303 ]\n",
      "F1-score per class: [0.55693867 0.60898542]\n",
      "Evaluation for Smoothing:\n",
      "Cross-Validation Accuracy: 0.5933876811594203\n",
      "Precision per class: [0.59585311 0.59481052]\n",
      "Recall per class: [0.54242424 0.6407197 ]\n",
      "F1-score per class: [0.56007546 0.61027071]\n",
      "Evaluation for Normalization:\n",
      "Cross-Validation Accuracy: 0.5933876811594203\n",
      "Precision per class: [0.59585311 0.59481052]\n",
      "Recall per class: [0.54242424 0.6407197 ]\n",
      "F1-score per class: [0.56007546 0.61027071]\n",
      "Evaluation for 1-Derivative Spectroscopy:\n",
      "Cross-Validation Accuracy: 0.8411684782608695\n",
      "Precision per class: [0.85779591 0.85701772]\n",
      "Recall per class: [0.83712121 0.84223485]\n",
      "F1-score per class: [0.83664748 0.83837128]\n",
      "Evaluation for 2-Derivative Spectroscopy:\n",
      "Cross-Validation Accuracy: 0.8201539855072465\n",
      "Precision per class: [0.82617945 0.84046084]\n",
      "Recall per class: [0.81287879 0.82518939]\n",
      "F1-score per class: [0.80915637 0.82159749]\n",
      "Evaluation for 1-SG-Derivative Spectroscopy:\n",
      "Cross-Validation Accuracy: 0.8316576086956522\n",
      "Precision per class: [0.84871568 0.84727451]\n",
      "Recall per class: [0.83143939 0.82897727]\n",
      "F1-score per class: [0.82916126 0.82644445]\n",
      "Evaluation for 2-SG-Derivative Spectroscopy:\n",
      "Cross-Validation Accuracy: 0.8287137681159422\n",
      "Precision per class: [0.83822641 0.8541439 ]\n",
      "Recall per class: [0.82954545 0.82613636]\n",
      "F1-score per class: [0.82112861 0.82678471]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate each preprocessing method\n",
    "print(\"Evaluation for No Preprocessing:\")\n",
    "classify_and_evaluate(df_0_selected_regions, columns_to_focus)\n",
    "\n",
    "print(\"Evaluation for Baseline Correction:\")\n",
    "df_baseline_corrected_v02 = pd.read_csv('data/data file 2b/data_1_bslcrct.csv')\n",
    "classify_and_evaluate(df_baseline_corrected_v02, columns_to_focus)\n",
    "\n",
    "print(\"Evaluation for Smoothing:\")\n",
    "df_smoothed_v02 = pd.read_csv('data/data file 2b/data_1_smoothed.csv')\n",
    "classify_and_evaluate(df_smoothed_v02, columns_to_focus)\n",
    "\n",
    "print(\"Evaluation for Normalization:\")\n",
    "df_normalized_v02 = pd.read_csv('data/data file 2b/data_1_normalized.csv')\n",
    "classify_and_evaluate(df_normalized_v02, columns_to_focus)\n",
    "\n",
    "print(\"Evaluation for 1-Derivative Spectroscopy:\")\n",
    "df_derivative_v02 = pd.read_csv('data/data file 2b/data_1_1_der.csv')\n",
    "classify_and_evaluate(df_derivative_v02, columns_to_focus)\n",
    "\n",
    "print(\"Evaluation for 2-Derivative Spectroscopy:\")\n",
    "data_2_der_combined_v02 = pd.read_csv('data/data file 2b/data_1_2_der.csv')\n",
    "classify_and_evaluate(data_2_der_combined_v02, columns_to_focus)\n",
    "\n",
    "print(\"Evaluation for 1-SG-Derivative Spectroscopy:\")\n",
    "data_1_der_savgol_combined_v02 = pd.read_csv('data/data file 2b/data_1_1_der_savgol.csv')\n",
    "classify_and_evaluate(data_1_der_savgol_combined_v02, columns_to_focus)\n",
    "\n",
    "print(\"Evaluation for 2-SG-Derivative Spectroscopy:\")\n",
    "data_2_der_savgol_combined_v02 = pd.read_csv('data/data file 2b/data_1_2_der_savgol.csv')\n",
    "classify_and_evaluate(data_2_der_savgol_combined_v02, columns_to_focus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for SNV:\n",
      "Cross-Validation Accuracy: 0.6871829710144928\n",
      "Precision per class: [0.7089437  0.67677392]\n",
      "Recall per class: [0.64772727 0.72613636]\n",
      "F1-score per class: [0.67206709 0.69703752]\n",
      "Evaluation for RNV:\n",
      "Cross-Validation Accuracy: 0.5071557971014492\n",
      "Precision per class: [0.51623482 0.49980187]\n",
      "Recall per class: [0.46060606 0.55113636]\n",
      "F1-score per class: [0.47596215 0.51654288]\n",
      "Evaluation for MSC:\n",
      "Cross-Validation Accuracy: 0.6871829710144928\n",
      "Precision per class: [0.7089437  0.67677392]\n",
      "Recall per class: [0.64772727 0.72613636]\n",
      "F1-score per class: [0.67206709 0.69703752]\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluation for SNV:\")\n",
    "df_snv_v02 = pd.read_csv('data/data file 2b/data_1_snv.csv')\n",
    "classify_and_evaluate(df_snv_v02, columns_to_focus)\n",
    "\n",
    "print(\"Evaluation for RNV:\")\n",
    "df_rnv_v02 = pd.read_csv('data/data file 2b/data_1_rnv.csv')\n",
    "classify_and_evaluate(df_rnv_v02, columns_to_focus)\n",
    "\n",
    "print(\"Evaluation for MSC:\")\n",
    "df_msc_v02 = pd.read_csv('data/data file 2b/data_1_msc.csv')\n",
    "classify_and_evaluate(df_msc_v02, columns_to_focus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification and Evaluation using LOGO-CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification and evaluation function using LOGO-CV with detailed metrics\n",
    "def classify_and_evaluate_logo_cv_detailed(df, columns):\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.model_selection import LeaveOneGroupOut\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "    \n",
    "    # Encode target variable\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(df[target])\n",
    "    \n",
    "    # Define features\n",
    "    X = df[columns]\n",
    "    \n",
    "    # Initialize the classifier\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    \n",
    "    # Initialize LOGO-CV\n",
    "    logo = LeaveOneGroupOut()\n",
    "    groups = df['prov_char']\n",
    "    \n",
    "    # Arrays to store results\n",
    "    accuracies = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "    all_y_test = []\n",
    "    all_y_pred = []\n",
    "    \n",
    "    # Perform LOGO-CV\n",
    "    for train_idx, test_idx in logo.split(X, y, groups=groups):\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "        \n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict and evaluate\n",
    "        y_pred = model.predict(X_test)\n",
    "        accuracies.append(accuracy_score(y_test, y_pred))\n",
    "        all_y_test.extend(y_test)\n",
    "        all_y_pred.extend(y_pred)\n",
    "        \n",
    "    # Calculate overall metrics\n",
    "    mean_accuracy = np.mean(accuracies)\n",
    "    precision = precision_score(all_y_test, all_y_pred, average=None)\n",
    "    recall = recall_score(all_y_test, all_y_pred, average=None)\n",
    "    f1 = f1_score(all_y_test, all_y_pred, average=None)\n",
    "    conf_matrix = confusion_matrix(all_y_test, all_y_pred)\n",
    "    \n",
    "    # Print the results\n",
    "    print(f'Mean Accuracy: {mean_accuracy}')\n",
    "    print(f'Precision per class: {precision}')\n",
    "    print(f'Recall per class: {recall}')\n",
    "    print(f'F1-score per class: {f1}')\n",
    "    \n",
    "    # Return confusion matrix for presentation\n",
    "    return conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOGO-CV Evaluation for No Preprocessing:\n",
      "Mean Accuracy: 0.5801241907237283\n",
      "Precision per class: [0.49579832 0.4974271 ]\n",
      "Recall per class: [0.37659574 0.61702128]\n",
      "F1-score per class: [0.4280532  0.55080722]\n",
      "LOGO-CV Evaluation for Baseline Correction:\n",
      "Mean Accuracy: 0.5074279246074703\n",
      "Precision per class: [0.39304813 0.42932862]\n",
      "Recall per class: [0.31276596 0.51702128]\n",
      "F1-score per class: [0.34834123 0.46911197]\n",
      "LOGO-CV Evaluation for Smoothing:\n",
      "Mean Accuracy: 0.575664986849123\n",
      "Precision per class: [0.47869674 0.48428835]\n",
      "Recall per class: [0.40638298 0.55744681]\n",
      "F1-score per class: [0.43958573 0.51829871]\n",
      "LOGO-CV Evaluation for Normalization:\n",
      "Mean Accuracy: 0.5753659437869222\n",
      "Precision per class: [0.47738693 0.48339483]\n",
      "Recall per class: [0.40425532 0.55744681]\n",
      "F1-score per class: [0.43778802 0.51778656]\n",
      "LOGO-CV Evaluation for 1-Derivative Spectroscopy:\n",
      "Mean Accuracy: 0.8545845339928044\n",
      "Precision per class: [0.80472103 0.79957806]\n",
      "Recall per class: [0.79787234 0.80638298]\n",
      "F1-score per class: [0.80128205 0.8029661 ]\n",
      "LOGO-CV Evaluation for 2-Derivative Spectroscopy:\n",
      "Mean Accuracy: 0.8251560993023381\n",
      "Precision per class: [0.79340659 0.77525773]\n",
      "Recall per class: [0.76808511 0.8       ]\n",
      "F1-score per class: [0.78054054 0.78743455]\n",
      "LOGO-CV Evaluation for 1-SG-Derivative Spectroscopy:\n",
      "Mean Accuracy: 0.8442939737730978\n",
      "Precision per class: [0.79520697 0.78170478]\n",
      "Recall per class: [0.77659574 0.8       ]\n",
      "F1-score per class: [0.78579117 0.79074658]\n",
      "LOGO-CV Evaluation for 2-SG-Derivative Spectroscopy:\n",
      "Mean Accuracy: 0.8252425779026269\n",
      "Precision per class: [0.79605263 0.77892562]\n",
      "Recall per class: [0.77234043 0.80212766]\n",
      "F1-score per class: [0.78401728 0.79035639]\n",
      "LOGO-CV Evaluation for SNV:\n",
      "Mean Accuracy: 0.5801241907237283\n",
      "Precision per class: [0.49579832 0.4974271 ]\n",
      "Recall per class: [0.37659574 0.61702128]\n",
      "F1-score per class: [0.4280532  0.55080722]\n",
      "LOGO-CV Evaluation for RNV:\n",
      "Mean Accuracy: 0.36781499010646923\n",
      "Precision per class: [0.21100917 0.25      ]\n",
      "Recall per class: [0.19574468 0.26808511]\n",
      "F1-score per class: [0.20309051 0.2587269 ]\n",
      "LOGO-CV Evaluation for MSC:\n",
      "Mean Accuracy: 0.5801241907237283\n",
      "Precision per class: [0.49579832 0.4974271 ]\n",
      "Recall per class: [0.37659574 0.61702128]\n",
      "F1-score per class: [0.4280532  0.55080722]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate each preprocessing method using LOGO-CV with detailed metrics\n",
    "print(\"LOGO-CV Evaluation for No Preprocessing:\")\n",
    "conf_matrix_no_preprocessing_v03 = classify_and_evaluate_logo_cv_detailed(df_0_selected_regions, columns_to_focus)\n",
    "\n",
    "print(\"LOGO-CV Evaluation for Baseline Correction:\")\n",
    "df_baseline_corrected_v03 = pd.read_csv('data/data file 2b/data_1_bslcrct.csv')\n",
    "conf_matrix_baseline_v03 = classify_and_evaluate_logo_cv_detailed(df_baseline_corrected_v03, columns_to_focus)\n",
    "\n",
    "print(\"LOGO-CV Evaluation for Smoothing:\")\n",
    "df_smoothed_v03 = pd.read_csv('data/data file 2b/data_1_smoothed.csv')\n",
    "conf_matrix_smoothing_v03 = classify_and_evaluate_logo_cv_detailed(df_smoothed_v03, columns_to_focus)\n",
    "\n",
    "print(\"LOGO-CV Evaluation for Normalization:\")\n",
    "df_normalized_v03 = pd.read_csv('data/data file 2b/data_1_normalized.csv')\n",
    "conf_matrix_normalization_v03 = classify_and_evaluate_logo_cv_detailed(df_normalized_v03, columns_to_focus)\n",
    "\n",
    "print(\"LOGO-CV Evaluation for 1-Derivative Spectroscopy:\")\n",
    "df_1_derivative_v03 = pd.read_csv('data/data file 2b/data_1_1_der.csv')\n",
    "conf_matrix_1_derivative_v03 = classify_and_evaluate_logo_cv_detailed(df_1_derivative_v03, columns_to_focus)\n",
    "\n",
    "print(\"LOGO-CV Evaluation for 2-Derivative Spectroscopy:\")\n",
    "df_2_derivative_v03 = pd.read_csv('data/data file 2b/data_1_2_der.csv')\n",
    "conf_matrix_2_derivative_v03 = classify_and_evaluate_logo_cv_detailed(df_2_derivative_v03, columns_to_focus)\n",
    "\n",
    "print(\"LOGO-CV Evaluation for 1-SG-Derivative Spectroscopy:\")\n",
    "df_1_der_savgol_combined_v03 = pd.read_csv('data/data file 2b/data_1_1_der_savgol.csv')\n",
    "conf_matrix_1_sg_v03 = classify_and_evaluate_logo_cv_detailed(df_1_der_savgol_combined_v03, columns_to_focus)\n",
    "\n",
    "print(\"LOGO-CV Evaluation for 2-SG-Derivative Spectroscopy:\")\n",
    "df_2_der_savgol_combined_v03 = pd.read_csv('data/data file 2b/data_1_2_der_savgol.csv')\n",
    "conf_matrix_2_sg_v03 = classify_and_evaluate_logo_cv_detailed(df_2_der_savgol_combined_v03, columns_to_focus)\n",
    "\n",
    "print(\"LOGO-CV Evaluation for SNV:\")\n",
    "df_snv_v03 = pd.read_csv('data/data file 2b/data_1_snv.csv')\n",
    "conf_matrix_snv_v03 = classify_and_evaluate_logo_cv_detailed(df_snv_v03, columns_to_focus)\n",
    "\n",
    "print(\"LOGO-CV Evaluation for RNV:\")\n",
    "df_rnv_v03 = pd.read_csv('data/data file 2b/data_1_rnv.csv')\n",
    "conf_matrix_rnv_v03 = classify_and_evaluate_logo_cv_detailed(df_rnv_v03, columns_to_focus)\n",
    "\n",
    "print(\"LOGO-CV Evaluation for MSC:\")\n",
    "df_msc_v03 = pd.read_csv('data/data file 2b/data_1_msc.csv')\n",
    "conf_matrix_msc_v03 = classify_and_evaluate_logo_cv_detailed(df_msc_v03, columns_to_focus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display confusion matrix in a tabular format\n",
    "def display_confusion_matrix(conf_matrix, class_labels):\n",
    "    df_cm = pd.DataFrame(conf_matrix, index=class_labels, columns=class_labels)\n",
    "    print(df_cm)\n",
    "\n",
    "# Ensure LabelEncoder is defined and class labels are set\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Define the target variable and fit LabelEncoder\n",
    "target = 'thnoth_name'\n",
    "le = LabelEncoder()\n",
    "le.fit(df_0_selected_regions[target])\n",
    "class_labels = le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for No Preprocessing:\n",
      "          Non-Thai  Thai\n",
      "Non-Thai       177   293\n",
      "Thai           180   290\n",
      "Confusion Matrix for Baseline Correction:\n",
      "          Non-Thai  Thai\n",
      "Non-Thai       147   323\n",
      "Thai           227   243\n",
      "Confusion Matrix for Smoothing:\n",
      "          Non-Thai  Thai\n",
      "Non-Thai       191   279\n",
      "Thai           208   262\n",
      "Confusion Matrix for Normalization:\n",
      "          Non-Thai  Thai\n",
      "Non-Thai       190   280\n",
      "Thai           208   262\n",
      "Confusion Matrix for Derivative Spectroscopy:\n",
      "          Non-Thai  Thai\n",
      "Non-Thai       375    95\n",
      "Thai            91   379\n",
      "Confusion Matrix for Derivative Spectroscopy:\n",
      "          Non-Thai  Thai\n",
      "Non-Thai       361   109\n",
      "Thai            94   376\n",
      "Confusion Matrix for Derivative Spectroscopy:\n",
      "          Non-Thai  Thai\n",
      "Non-Thai       365   105\n",
      "Thai            94   376\n",
      "Confusion Matrix for Derivative Spectroscopy:\n",
      "          Non-Thai  Thai\n",
      "Non-Thai       363   107\n",
      "Thai            93   377\n",
      "Confusion Matrix for SNV:\n",
      "          Non-Thai  Thai\n",
      "Non-Thai       177   293\n",
      "Thai           180   290\n",
      "Confusion Matrix for RNV:\n",
      "          Non-Thai  Thai\n",
      "Non-Thai        92   378\n",
      "Thai           344   126\n",
      "Confusion Matrix for MSC:\n",
      "          Non-Thai  Thai\n",
      "Non-Thai       177   293\n",
      "Thai           180   290\n"
     ]
    }
   ],
   "source": [
    "# Display confusion matrices for each preprocessing method\n",
    "print(\"Confusion Matrix for No Preprocessing:\")\n",
    "display_confusion_matrix(conf_matrix_no_preprocessing_v03, class_labels)\n",
    "\n",
    "print(\"Confusion Matrix for Baseline Correction:\")\n",
    "display_confusion_matrix(conf_matrix_baseline_v03, class_labels)\n",
    "\n",
    "print(\"Confusion Matrix for Smoothing:\")\n",
    "display_confusion_matrix(conf_matrix_smoothing_v03, class_labels)\n",
    "\n",
    "print(\"Confusion Matrix for Normalization:\")\n",
    "display_confusion_matrix(conf_matrix_normalization_v03, class_labels)\n",
    "\n",
    "print(\"Confusion Matrix for Derivative Spectroscopy:\")\n",
    "display_confusion_matrix(conf_matrix_1_derivative_v03, class_labels)\n",
    "\n",
    "print(\"Confusion Matrix for Derivative Spectroscopy:\")\n",
    "display_confusion_matrix(conf_matrix_2_derivative_v03, class_labels)\n",
    "\n",
    "print(\"Confusion Matrix for Derivative Spectroscopy:\")\n",
    "display_confusion_matrix(conf_matrix_1_sg_v03, class_labels)\n",
    "\n",
    "print(\"Confusion Matrix for Derivative Spectroscopy:\")\n",
    "display_confusion_matrix(conf_matrix_2_sg_v03, class_labels)\n",
    "\n",
    "print(\"Confusion Matrix for SNV:\")\n",
    "display_confusion_matrix(conf_matrix_snv_v03, class_labels)\n",
    "\n",
    "print(\"Confusion Matrix for RNV:\")\n",
    "display_confusion_matrix(conf_matrix_rnv_v03, class_labels)\n",
    "\n",
    "print(\"Confusion Matrix for MSC:\")\n",
    "display_confusion_matrix(conf_matrix_msc_v03, class_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
